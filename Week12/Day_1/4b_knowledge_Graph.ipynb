{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import phidata\n",
    "Install the phidata library and import necessary modules for working with agents, tools, knowledge, and storage components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy==1.26.4 ipykernel phidata openai ipywidgets duckduckgo-search yfinance crawl4ai lancedb sentence-transformers torch pypdf chromadb duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "1. **Importing Required Modules**:\n",
    "    - `os`: Provides functions to interact with the operating system.\n",
    "    - `load_dotenv` from `dotenv`: Loads environment variables from a `.env` file into the system environment.\n",
    "\n",
    "2. **Loading Environment Variables**:\n",
    "    - The `load_dotenv()` function is called to load environment variables from a `.env` file.\n",
    "\n",
    "3. **Clearing Conflicting Environment Variables**:\n",
    "    - A list of environment variables (`env_vars_to_clear`) is defined, which includes `OPENAI_API_KEY`, `OPENAI_BASE_URL`, and `OPENAI_API_BASE`.\n",
    "    - For each variable in the list, the code checks if it exists in the environment. If it does, it prints a warning message and deletes the variable from the environment.\n",
    "\n",
    "4. **Setting New Environment Variables**:\n",
    "    - The `OPENAI_API_KEY` is set to the value of the `OPEN_ROUTER_KEY` environment variable.\n",
    "    - Both `OPENAI_API_BASE` and `OPENAI_BASE_URL` are set to `'https://openrouter.ai/api/v1'`.\n",
    "\n",
    "5. **Commented-Out Code**:\n",
    "    - There is an alternative block of code (commented out) that clears the same environment variables and sets `OPENAI_API_KEY` to the value of the `OPEN_AI_KEY` environment variable instead.\n",
    "\n",
    "### Purpose:\n",
    "This code ensures that conflicting environment variables are removed and sets up the required environment variables for interacting with the OpenRouter API. It provides flexibility to switch between different API keys and base URLs by modifying the `.env` file or uncommenting the alternative block of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "env_vars_to_clear = ['OPENAI_API_KEY', 'OPENAI_BASE_URL', 'OPENAI_API_BASE']\n",
    "for var in env_vars_to_clear:\n",
    "    if os.getenv(var):\n",
    "        print(f\"⚠️  Removing conflicting {var}\")\n",
    "        del os.environ[var]\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPEN_ROUTER_KEY\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPEN_AI_KEY\")\n",
    "os.environ['OPENAI_API_BASE'] = 'https://openrouter.ai/api/v1'\n",
    "os.environ['OPENAI_BASE_URL'] = 'https://openrouter.ai/api/v1'\n",
    "\n",
    "\n",
    "\n",
    "# env_vars_to_clear = ['OPENAI_API_KEY', 'OPENAI_BASE_URL', 'OPENAI_API_BASE']\n",
    "# for var in env_vars_to_clear:\n",
    "#     if os.getenv(var):\n",
    "#         print(f\"⚠️  Removing conflicting {var}\")\n",
    "#         del os.environ[var]\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPEN_AI_KEY\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "The code in the next cell performs the following steps:\n",
    "\n",
    "1. **Importing Required Modules**:\n",
    "    - `phi.agent.Agent`: Used to create an agent that can perform reasoning tasks.\n",
    "    - `phi.tools.Tool`: Provides tools that can be used by the agent.\n",
    "    - `phi.model.openai.OpenAIChat`: Represents the OpenAI GPT model used for generating responses.\n",
    "    - `phi.utils.pprint.pprint_run_response`: A utility function to pretty-print the agent's response.\n",
    "\n",
    "### Purpose:\n",
    "This code sets up the necessary imports to create and interact with agents that utilize OpenAI's GPT models for reasoning and task execution. It also includes tools and utilities for enhancing the agent's functionality and formatting its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi.agent import Agent\n",
    "from phi.tools import Tool\n",
    "from phi.model.openai import OpenAIChat\n",
    "from phi.utils.pprint import pprint_run_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from phi.agent import Agent\n",
    "from phi.model.openai import OpenAIChat\n",
    "from phi.tools.csv_tools import CsvTools\n",
    "from phi.tools.python import PythonTools\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "\n",
    "class KnowledgeGraphQueryTools:\n",
    "    \"\"\"Custom tools for querying the knowledge graph\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = None\n",
    "        self.df = None\n",
    "    \n",
    "    def load_graph_and_data(self, graph_path: str = \"movie_kg.graphml\", csv_path: str = \"imdb.csv\"):\n",
    "        \"\"\"Load the knowledge graph and original data\"\"\"\n",
    "        try:\n",
    "            self.graph = nx.read_graphml(graph_path)\n",
    "            self.df = pd.read_csv(csv_path)\n",
    "            return f\"Loaded graph with {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges\"\n",
    "        except Exception as e:\n",
    "            return f\"Error loading graph: {str(e)}\"\n",
    "    \n",
    "    def query_node_relationships(self, node_name: str, max_depth: int = 2):\n",
    "        \"\"\"Find all relationships for a node up to specified depth\"\"\"\n",
    "        if not self.graph:\n",
    "            return \"Graph not loaded. Please load the graph first.\"\n",
    "        \n",
    "        if node_name not in self.graph:\n",
    "            return f\"Node '{node_name}' not found in graph\"\n",
    "        \n",
    "        relationships = {}\n",
    "        for depth in range(1, max_depth + 1):\n",
    "            nodes_at_depth = []\n",
    "            for node in nx.single_source_shortest_path_length(self.graph, node_name, cutoff=depth):\n",
    "                if nx.shortest_path_length(self.graph, node_name, node) == depth:\n",
    "                    nodes_at_depth.append(node)\n",
    "            relationships[f\"depth_{depth}\"] = nodes_at_depth\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def find_common_connections(self, node1: str, node2: str):\n",
    "        \"\"\"Find common connections between two nodes\"\"\"\n",
    "        if not self.graph:\n",
    "            return \"Graph not loaded. Please load the graph first.\"\n",
    "        \n",
    "        neighbors1 = set(self.graph.neighbors(node1))\n",
    "        neighbors2 = set(self.graph.neighbors(node2))\n",
    "        common = neighbors1.intersection(neighbors2)\n",
    "        \n",
    "        return {\n",
    "            \"common_connections\": list(common),\n",
    "            \"node1_unique\": list(neighbors1 - neighbors2),\n",
    "            \"node2_unique\": list(neighbors2 - neighbors1)\n",
    "        }\n",
    "    \n",
    "    def analyze_node_centrality(self, node_type: str = None):\n",
    "        \"\"\"Analyze centrality measures for nodes\"\"\"\n",
    "        if not self.graph:\n",
    "            return \"Graph not loaded. Please load the graph first.\"\n",
    "        \n",
    "        centrality_measures = {\n",
    "            \"degree\": nx.degree_centrality(self.graph),\n",
    "            \"betweenness\": nx.betweenness_centrality(self.graph),\n",
    "            \"closeness\": nx.closeness_centrality(self.graph),\n",
    "            \"eigenvector\": nx.eigenvector_centrality(self.graph, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        # Filter by node type if specified\n",
    "        if node_type:\n",
    "            filtered_measures = {}\n",
    "            for measure_name, measure_dict in centrality_measures.items():\n",
    "                filtered_measures[measure_name] = {\n",
    "                    node: score for node, score in measure_dict.items() \n",
    "                    if self.graph.nodes[node].get('type') == node_type\n",
    "                }\n",
    "            return filtered_measures\n",
    "        \n",
    "        return centrality_measures\n",
    "    \n",
    "    def find_shortest_path_with_context(self, source: str, target: str):\n",
    "        \"\"\"Find shortest path with additional context about the relationships\"\"\"\n",
    "        if not self.graph:\n",
    "            return \"Graph not loaded. Please load the graph first.\"\n",
    "        \n",
    "        try:\n",
    "            path = nx.shortest_path(self.graph, source, target)\n",
    "            path_details = []\n",
    "            \n",
    "            for i in range(len(path) - 1):\n",
    "                edge_data = self.graph.get_edge_data(path[i], path[i+1])\n",
    "                path_details.append({\n",
    "                    \"from\": path[i],\n",
    "                    \"to\": path[i+1],\n",
    "                    \"relationship\": edge_data\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                \"path\": path,\n",
    "                \"path_length\": len(path) - 1,\n",
    "                \"path_details\": path_details\n",
    "            }\n",
    "        except nx.NetworkXNoPath:\n",
    "            return f\"No path found between {source} and {target}\"\n",
    "\n",
    "# Create enhanced agent with query capabilities\n",
    "def create_enhanced_kg_agent():\n",
    "    imdb_csv = \"imdb.csv\"\n",
    "    \n",
    "    # Initialize query tools\n",
    "    query_tools = KnowledgeGraphQueryTools()\n",
    "    \n",
    "    return Agent(\n",
    "        name=\"Enhanced IMDB Knowledge Graph Agent\",\n",
    "        # model=OpenAIChat(id=\"gpt-4o\"),\n",
    "        model=OpenAIChat(id=\"gpt-4-turbo-preview\"),\n",
    "        tools=[\n",
    "            CsvTools(csvs=[imdb_csv]),\n",
    "            PythonTools(pip_install=True, save_and_run=True)\n",
    "        ],\n",
    "        instructions=[\n",
    "            \"You are an expert at analyzing movie data, creating knowledge graphs, and answering complex queries\",\n",
    "            \"You can handle both first-order queries (direct facts) and second-order queries (relationships between relationships)\",\n",
    "            \"Always provide comprehensive analysis with visualizations when appropriate\",\n",
    "            \n",
    "            # Knowledge Graph Creation Instructions\n",
    "            \"For graph creation:\",\n",
    "            \"- First, explore the CSV file structure\",\n",
    "            \"- Create nodes for Directors, Genres, Actors, and Movies\",\n",
    "            \"- Add edges with weights based on ratings/revenue\",\n",
    "            \"- Save the graph as GraphML format for later querying\",\n",
    "            \n",
    "            # Second-Order Query Instructions  \n",
    "            \"For second-order queries, you can analyze:\",\n",
    "            \"- Indirect relationships (who worked with whom through common projects)\",\n",
    "            \"- Network patterns (clusters, communities, influential nodes)\",\n",
    "            \"- Path analysis (how entities are connected through intermediaries)\", \n",
    "            \"- Comparative analysis (similarities between different entities)\",\n",
    "            \"- Temporal patterns (if time data is available)\",\n",
    "            \n",
    "            # Available query functions\n",
    "            \"Available query functions:\",\n",
    "            \"- query_node_relationships(): Find multi-hop relationships\",\n",
    "            \"- find_common_connections(): Discover shared connections\",\n",
    "            \"- analyze_node_centrality(): Identify most influential nodes\",\n",
    "            \"- find_shortest_path_with_context(): Analyze connection paths\",\n",
    "            \n",
    "            \"Always visualize results when possible and provide actionable insights\"\n",
    "            \"save visuals as png files and return the file paths\",\n",
    "            \"Use networkx for graph operations and matplotlib for visualizations\",\n",
    "        ],\n",
    "        markdown=True,\n",
    "        show_tool_calls=True,\n",
    "    )\n",
    "\n",
    "# Create the enhanced agent\n",
    "knowledge_graph_agent = create_enhanced_kg_agent()\n",
    "\n",
    "# Initial knowledge graph creation\n",
    "print(\"=== PHASE 1: Creating Knowledge Graph ===\")\n",
    "knowledge_graph_agent.print_response(\"\"\"\n",
    "Use only 200 records from the IMDB dataset.\n",
    "\n",
    "Please analyze the IMDB movie dataset and create a knowledge graph:\n",
    "\n",
    "1. First, examine the CSV file structure and show columns/sample data\n",
    "2. Create a knowledge graph with:\n",
    "   - Directors as nodes  \n",
    "   - Genres as nodes\n",
    "   - Actors as nodes\n",
    "   - Movies as nodes\n",
    "   - Edge weights based on movie ratings or revenue\n",
    "\n",
    "3. Visualize using networkx and matplotlib\n",
    "4. IMPORTANT: Save the graph as 'movie_kg.graphml' for later querying\n",
    "5. Provide initial insights about most connected nodes\n",
    "\n",
    "Use appropriate graph layout algorithms for clear visualization.\n",
    "\"\"\", stream=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== PHASE 2: Second-Order Query Examples ===\")\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example second-order queries\n",
    "second_order_queries = [\n",
    "    \"\"\"\n",
    "    QUERY 1 - Indirect Collaborations:\n",
    "    Find directors who have never worked directly together but have worked with the same actors. \n",
    "    Show the \"degrees of separation\" between directors through their shared cast members.\n",
    "    Visualize this as a network showing indirect connections.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    QUERY 2 - Genre Bridge Analysis:\n",
    "    Identify actors or directors who serve as \"bridges\" between different genres.\n",
    "    Find who connects seemingly unrelated genres (e.g., horror and comedy).\n",
    "    Show the shortest paths between genre clusters.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    QUERY 3 - Influence Propagation:\n",
    "    Analyze how high-rated movies influence the rating patterns of connected movies.\n",
    "    Do actors/directors from highly-rated films tend to be in other highly-rated films?\n",
    "    Create a visualization showing this influence network.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    QUERY 4 - Community Detection:\n",
    "    Find clusters or communities in the movie network.\n",
    "    Identify groups of actors/directors who frequently work together.\n",
    "    Analyze if these communities have distinct characteristics (genres, ratings, time periods).\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute second-order queries\n",
    "for i, query in enumerate(second_order_queries, 1):\n",
    "    print(f\"\\n--- Executing Query {i} ---\")\n",
    "    knowledge_graph_agent.print_response(query, stream=True)\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "\n",
    "# Interactive query function\n",
    "def interactive_query():\n",
    "    \"\"\"Allow users to input custom second-order queries\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"=== INTERACTIVE SECOND-ORDER QUERY MODE ===\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"You can now ask complex questions about the movie knowledge graph!\")\n",
    "    print(\"Examples of second-order queries you can ask:\")\n",
    "    print(\"- 'How are action movies connected to comedy movies through shared actors?'\")\n",
    "    print(\"- 'Which director has the most indirect influence in the network?'\")\n",
    "    print(\"- 'Find the shortest collaboration path between two specific actors'\")\n",
    "    print(\"- 'What genres tend to cluster together through shared personnel?'\")\n",
    "    print(\"\\nType 'exit' to quit interactive mode\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"\\nEnter your second-order query: \").strip()\n",
    "        if user_query.lower() == 'exit':\n",
    "            break\n",
    "        if user_query:\n",
    "            print(f\"\\n--- Processing Query: {user_query} ---\")\n",
    "            knowledge_graph_agent.print_response(user_query, stream=True)\n",
    "\n",
    "# Uncomment the line below to enable interactive querying\n",
    "# interactive_query()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Knowledge Graph Agent with Second-Order Queries Ready!\")\n",
    "print(\"The agent can now handle complex relational queries beyond simple facts.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from phi.agent import Agent\n",
    "from phi.model.openai import OpenAIChat\n",
    "from phi.tools.csv_tools import CsvTools\n",
    "from phi.tools.python import PythonTools\n",
    "\n",
    "class GraphQueryTools:\n",
    "    def __init__(self):\n",
    "        self.graph = None\n",
    "        self.df = None\n",
    "    \n",
    "    def load_graph(self, graph_path=\"movie_kg.graphml\", csv_path=\"imdb.csv\"):\n",
    "        try:\n",
    "            self.graph = nx.read_graphml(graph_path)\n",
    "            self.df = pd.read_csv(csv_path)\n",
    "            return f\"Loaded: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges\"\n",
    "        except:\n",
    "            return \"Graph not found. Create it first.\"\n",
    "    \n",
    "    def find_indirect_connections(self, node1, node2, max_depth=3):\n",
    "        if not self.graph or node1 not in self.graph or node2 not in self.graph:\n",
    "            return \"Invalid nodes or graph not loaded\"\n",
    "        \n",
    "        try:\n",
    "            path = nx.shortest_path(self.graph, node1, node2)\n",
    "            return {\"path\": path, \"length\": len(path)-1}\n",
    "        except:\n",
    "            return \"No connection found\"\n",
    "    \n",
    "    def common_neighbors(self, node1, node2):\n",
    "        if not self.graph:\n",
    "            return \"Graph not loaded\"\n",
    "        \n",
    "        n1_neighbors = set(self.graph.neighbors(node1))\n",
    "        n2_neighbors = set(self.graph.neighbors(node2))\n",
    "        common = n1_neighbors.intersection(n2_neighbors)\n",
    "        \n",
    "        return list(common)\n",
    "    \n",
    "    def get_node_influence(self, node_type=None):\n",
    "        if not self.graph:\n",
    "            return \"Graph not loaded\"\n",
    "        \n",
    "        centrality = nx.degree_centrality(self.graph)\n",
    "        if node_type:\n",
    "            return {n: c for n, c in centrality.items() \n",
    "                   if self.graph.nodes[n].get('type') == node_type}\n",
    "        return centrality\n",
    "\n",
    "# Create streamlined agent\n",
    "imdb_csv = \"imdb.csv\"\n",
    "\n",
    "knowledge_graph_agent = Agent(\n",
    "    name=\"IMDB KG Agent\",\n",
    "    model=OpenAIChat(id=\"gpt-4o\"),\n",
    "    tools=[\n",
    "        CsvTools(csvs=[imdb_csv]),\n",
    "        PythonTools(pip_install=True, save_and_run=True)\n",
    "    ],\n",
    "    instructions=[\n",
    "        \"Analyze movie data and create knowledge graphs\",\n",
    "        \"Handle both direct and indirect relationship queries\",\n",
    "        \"Create nodes: Directors, Genres, Actors, Movies\",\n",
    "        \"Save graph as 'movie_kg.graphml' for querying\",\n",
    "        \"For second-order queries, analyze relationships between relationships\"\n",
    "    ],\n",
    "    markdown=True,\n",
    "    show_tool_calls=True,\n",
    ")\n",
    "\n",
    "# Phase 1: Create knowledge graph\n",
    "print(\"=== Creating Knowledge Graph ===\")\n",
    "knowledge_graph_agent.print_response(\"\"\"\n",
    "Use 200 records from IMDB dataset.\n",
    "\n",
    "1. Load and examine CSV structure\n",
    "2. Create knowledge graph with Directors, Genres, Actors, Movies as nodes\n",
    "3. Add edges with weights (ratings/revenue)\n",
    "4. Save as 'movie_kg.graphml'\n",
    "5. Visualize and show basic stats\n",
    "\"\"\", stream=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Second-order queries\n",
    "print(\"\\n=== Second-Order Queries ===\")\n",
    "\n",
    "queries = [\n",
    "    \"Find directors who never worked together but share common actors. Show indirect connections.\",\n",
    "    \"Identify actors who bridge different genres. Visualize genre connectivity.\",\n",
    "    \"Analyze influence patterns: do high-rated films connect to other high-rated films?\",\n",
    "    \"Detect communities of frequently collaborating personnel.\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n--- Query {i} ---\")\n",
    "    knowledge_graph_agent.print_response(query, stream=True)\n",
    "\n",
    "print(\"\\n=== Ready for Interactive Queries ===\")\n",
    "print(\"Agent can now handle complex relational questions about the movie network.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
