{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import lancedb\n",
    "import openai\n",
    "import groq\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Text splitter functionality is provided by LangChain framework\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Make use of BS for hadling the web content\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b763a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedder_1 = SentenceTransformer (\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "Embedder_2 = SentenceTransformer (\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "# Initialise an client object with API key\n",
    "load_dotenv ()\n",
    "Retrieval_Client = groq.Groq ()\n",
    "Gen_Client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Lance DB Vector Base\n",
    "DB = lancedb.connect ('Quick_Ref')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1362927",
   "metadata": {},
   "source": [
    "**Get Main Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f37e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_content (url, type):\n",
    "\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove layout elements\n",
    "    for tag in soup([\"nav\", \"header\", \"footer\", \"aside\", \"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Check and get main section of the pages\n",
    "    main = soup.find(\"main\")\n",
    "\n",
    "    if not main:\n",
    "        \n",
    "        # fallback method, if no 'main' section in html page\n",
    "        candidates = soup.find_all(\"div\", recursive=True)\n",
    "        main = max(candidates, key=lambda c: len(c.get_text(strip=True)), default=soup.body)\n",
    "\n",
    "    # Get cleaned HTML content. Tags retained\n",
    "    main_html = str(main)\n",
    "\n",
    "    # If HTML content is required, provide with the tags\n",
    "    if type == 'html':\n",
    "        return (main_html)\n",
    "\n",
    "    # If text is requirred, provide only the text content\n",
    "    elif type == 'text':\n",
    "\n",
    "        text_soup = BeautifulSoup (main_html, \"html.parser\")\n",
    "        main_text = text_soup.get_text(separator=\"\\n\", strip=True)\n",
    "        return main_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5efe9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_data_from_headings (heading: dict, n: int = 1, from_end: bool = True, sep: str = \" : \") -> str:\n",
    "    \"\"\"\n",
    "    Concatenates n values from a heading dictionary, either from the start or from the end.\n",
    "\n",
    "    Param:\n",
    "        heading (dict): Input dictionary for headings.\n",
    "        n (int): Number of elements to take.\n",
    "        from_end (bool): If True, take from the end; else from the start.\n",
    "        sep (str): Optional separator to use between concatenated strings.\n",
    "\n",
    "    Returns: Meta data as concatenation of headings.\n",
    "    \"\"\"\n",
    "    values = list(heading.values())\n",
    "\n",
    "    if n <= 0:\n",
    "        n = 1\n",
    "    if n > len(values):\n",
    "        n = len(values)\n",
    "\n",
    "    # Select n items from start or end\n",
    "    selected = values[-n:] if from_end else values[:n]\n",
    "\n",
    "    # Always concatenate in forward direction\n",
    "    return sep.join(str(v) for v in selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Chunks (url, source, chunk_size_limit = 500, chunk_size=300):\n",
    "\n",
    "    # Define what are the splitters to be considered. There is default in library itself\n",
    "    seperators = [\".\", \"?\", \"!\"]\n",
    "\n",
    "    # Splitter function based on seperator and the length criteria\n",
    "    text_splitter = RecursiveCharacterTextSplitter (chunk_size=chunk_size, chunk_overlap=0,\n",
    "                                                    length_function=len, is_separator_regex=False,\n",
    "                                                    keep_separator=False,\n",
    "                                                    separators=seperators,\n",
    "                                                    )\n",
    "\n",
    "    # levels of header tags in html to split on\n",
    "    header_levels = [\n",
    "        (\"h1\", \"Header 1\"),\n",
    "        (\"h2\", \"Header 2\"),\n",
    "        (\"h3\", \"Header 3\"),\n",
    "        (\"h4\", \"Header 4\"),\n",
    "    ]\n",
    "\n",
    "    # Define a Splitter object for HTML content from the lib\n",
    "    # This library also gives splitter for Markdown, JSON etc\n",
    "    html_splitter = HTMLHeaderTextSplitter(header_levels)    \n",
    "\n",
    "    # Get the main content\n",
    "    HTML_Content = get_main_content (url, \"html\")    \n",
    "\n",
    "    # Chunk based on document structure\n",
    "    docs = html_splitter.split_text (HTML_Content)\n",
    "\n",
    "    # Start with empty list\n",
    "    Chunks = []\n",
    "\n",
    "    with open ('chunks.txt', mode='w') as f:\n",
    "\n",
    "        for doc in docs :\n",
    "\n",
    "            try :\n",
    "\n",
    "                meta_data = meta_data_from_headings (doc.metadata)\n",
    "\n",
    "                if not meta_data:\n",
    "                    meta_data = 'Generic'\n",
    "\n",
    "                # If the chunk is too long,\n",
    "                if (len(doc.page_content) > chunk_size_limit):\n",
    "\n",
    "                    # Split by sentece(s) by shorter lenth\n",
    "                    splits = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "                    # Make them individual chunk with same meta data\n",
    "                    for split in splits:\n",
    "\n",
    "                        # Capture if the meta data and text are not the same\n",
    "                        if (meta_data != split):\n",
    "\n",
    "                            Chunk = {'source': source,'topic' : meta_data, 'text' : split}\n",
    "                            print (Chunk, \"\\n----\",file=f)\n",
    "\n",
    "                            Chunks = Chunks + [Chunk]\n",
    "                        \n",
    "                else :\n",
    "                    \n",
    "                    if (meta_data != doc.page_content):\n",
    "                        \n",
    "                        Chunk = {'source': source, 'topic' : meta_data, 'text' : doc.page_content}\n",
    "                        print (Chunk, \"\\n----\",file=f)\n",
    "                        Chunks = Chunks + [Chunk]\n",
    "                \n",
    "            except Exception :\n",
    "                pass\n",
    "\n",
    "    print (len(Chunks))\n",
    "    nb_chunks = len(Chunks)\n",
    "\n",
    "    return nb_chunks, Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Capture_Knowledge (url : str, table_name : str) -> dict[str, str] :\n",
    "\n",
    "    # Default Error Code\n",
    "    Ret_Val = {'Status' : \"Erorr Processing\"}\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    Source = parsed.netloc\n",
    "\n",
    "    # Generate Chunks from the website main content\n",
    "    nb_chunks, Chunks = Build_Chunks (url, Source)\n",
    "\n",
    "    # If there are no chunks, probably error getting the content\n",
    "    if nb_chunks <= 0 :\n",
    "        Ret_Val ['Status'] = \"No content at source\"\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Create vectors and store in the Chunks \n",
    "        for idx, Chunk in enumerate (Chunks):\n",
    "\n",
    "            vector = Embedder_1.encode (Chunk['text'])\n",
    "            Chunks[idx]['vector'] = vector.tolist ()\n",
    "        \n",
    "        # Create a Table and add the Chunks data\n",
    "        table = DB.create_table(table_name, data=Chunks, mode=\"overwrite\") \n",
    "        # print (table.schema)\n",
    "\n",
    "        # Status\n",
    "        Ret_Val ['Status'] = \"Knowledge Captured\"\n",
    "        Ret_Val ['Num_Chunks'] = str (nb_chunks)\n",
    "        Ret_Val ['Source'] = Source\n",
    "    \n",
    "    return Ret_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a82559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query transformation (LLM + fallback)\n",
    "\n",
    "def transform_query (query: str, n_paraphrases: int = 3) -> List[str]:\n",
    "    prompt = (\n",
    "        'You are given a user query. With that, produce:\\n'\n",
    "        f'1) a precise reformulation suitable for content retrieval (\"precise\")\\n'\n",
    "        f'2) {n_paraphrases} concise paraphrases of the original query suitable for semantic retrieval (\"paraphrases\")\\n'\n",
    "        'Return JSON with keys: \"precise\" (string) and \"paraphrases\" (list of strings). No additional Text\\n'\n",
    "        'User query: ' + query\n",
    "    )\n",
    "\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }\n",
    "    ]\n",
    "    completion = Retrieval_Client.chat.completions.create(\n",
    "        messages=messages,    \n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        # model=\"openai/gpt-oss-120b\",\n",
    "        temperature=0.0,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # print (completion.choices[0].message.content)\n",
    "\n",
    "    clean_str = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", completion.choices[0].message.content)\n",
    "    data = json.loads (clean_str)\n",
    "    texts = [data[\"precise\"]] + data[\"paraphrases\"]\n",
    "\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4653df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query (query: str, n_alternates: int = 3) -> List[str]:\n",
    "    prompt = (\n",
    "        f'Generate {n_alternates} diverse query variations that can expand search horizon, but preserve intent of the following user query:\\nQuery: {query}\\n'\n",
    "        'Return JSON with keys: \"alternates\" (list of strings). No additional Text\\n'\n",
    "        'User query: ' + query\n",
    "    )\n",
    "\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }\n",
    "    ]\n",
    "    completion = Retrieval_Client.chat.completions.create(\n",
    "        messages=messages,    \n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        # model=\"openai/gpt-oss-120b\",\n",
    "        temperature=0.0,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # print (completion.choices[0].message.content)\n",
    "\n",
    "    clean_str = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", completion.choices[0].message.content)\n",
    "    data = json.loads (clean_str)\n",
    "    texts = data[\"alternates\"]\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ask_Assistant (Query : str, table_name : str) -> str:\n",
    "    \n",
    "    # Instruction for the LLM\n",
    "    # Instruction = \"\"\"You are given context information and a user query. You have to provide detailed answer to user query based on information provided in context.\n",
    "    #                 Provide an informative answer to the user query **BASED** on the context.\n",
    "    #                 If sufficient details are not in context, respond as \"No Sufficient Details\"\n",
    "    #            \"\"\"\n",
    "\n",
    "    Instruction = \"\"\"You are given context information and a user query. You have to provide detailed answer to user query based on information provided in context.\n",
    "                    Provide an informative answer to the user query **BASED** on the context.\n",
    "                    Also provide reasoning about the answer based on the context as a \"Reason\" in 300 words.\n",
    "                    Response format : \n",
    "                    (Answer) \\n\\n To Give you Context :\\n (Reason).\n",
    "                \"\"\"\n",
    "        \n",
    "    # RAG Fusion\n",
    "    trans_queries = transform_query (Query)\n",
    "    expand_queries = expand_query (Query)\n",
    "\n",
    "    queries  = trans_queries + expand_queries\n",
    "\n",
    "    Context = []\n",
    "    table = DB.open_table (table_name)\n",
    "\n",
    "    for query in queries :\n",
    "\n",
    "        Query_Vector = Embedder_1.encode (query).tolist ()\n",
    "        Results = table.search(Query_Vector).distance_type(\"cosine\").distance_range(upper_bound=0.6).limit(5).to_list ()\n",
    "\n",
    "        # print (len (Results))\n",
    "\n",
    "        Text_List = [r['text'] for r in Results]\n",
    "        Context = Context + Text_List\n",
    "\n",
    "    Context = list(set(Context))\n",
    "    print (len(Context))\n",
    "\n",
    "    response = Gen_Client.models.generate_content(\n",
    "                    model=\"gemini-2.0-flash\",\n",
    "                    config =types.GenerateContentConfig(\n",
    "                                system_instruction=Instruction,\n",
    "                                # temperature=0.0\n",
    "                                ),\n",
    "                    contents = [\"Context : \\n\"+str(Context), \"User Query : \\n\"+Query]\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Table_Name = 'Temp'\n",
    "\n",
    "# ret = Capture_Knowledge (\"https://www.parlezvoustech.com/en/systemes-embarques-guide-debutants/\", Table_Name)\n",
    "# ret = Capture_Knowledge (\"https://www.freecodecamp.org/news/learn-embedded-systems-firmware-basics-handbook-for-devs/\", Table_Name)\n",
    "# ret = Capture_Knowledge (\"https://www.aopa.org/training-and-safety/online-learning/safety-spotlights/aircraft-systems/engine-basics\", Table_Name)\n",
    "ret = Capture_Knowledge (\"https://cleartax.in/s/income-tax\", Table_Name)\n",
    "# ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ac5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f213541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask your assitant\n",
    "Query = \"What is the information about?\"\n",
    "\n",
    "Resp = Ask_Assistant (Query, Table_Name)\n",
    "print (Resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Adv_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
