{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Dict, Any\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import lancedb\n",
        "import openai\n",
        "import groq\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "from google import genai\n",
        "from google.genai import types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inputs\n",
        "LANCE_DB_NAME = 'Vector_DB'\n",
        "LANCEDB_TABLE_NAME = 'tech_ref'\n",
        "TOP_K_PER_QUERY = 5\n",
        "MAX_COMBINED_RESULTS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Embedder_1 = SentenceTransformer (\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Embedder_2 = SentenceTransformer (\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "\n",
        "# Initialise an client object with API key\n",
        "load_dotenv ()\n",
        "Retrieval_Client = groq.Groq ()\n",
        "Gen_Client = genai.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e1876c",
      "metadata": {},
      "source": [
        "#### Utlities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "220924c2",
      "metadata": {},
      "source": [
        "**Query Keywords**  \n",
        "> Function to identify key words from the query    \n",
        "> This might help to retireve information based on the key word from the knowledge repo    \n",
        "> By this it enhances the envelop of search in repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify Keywords from Query\n",
        "\n",
        "def query_keywords (query: str, n_keywords: int = 3) -> List[str]:\n",
        "    prompt = (\n",
        "        'You are given a user query. With that, produce:\\n'        \n",
        "        f'{n_keywords} Individual Keywords that capture the crux of the query (\"keywords\").\\n'\n",
        "        'If there are fewer keywords than needed, Provide **ONLY** what is present. Dont Cookup'\n",
        "        'Return JSON with keys: \"keywords\" (list of strings).\\n'\n",
        "        'User query: ' + query\n",
        "    )\n",
        "\n",
        "    messages=[\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt,\n",
        "    }\n",
        "    ]\n",
        "    completion = Retrieval_Client.chat.completions.create(\n",
        "        messages=messages,    \n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        # model=\"openai/gpt-oss-120b\",\n",
        "        stop=None,\n",
        "    )\n",
        "\n",
        "    # print (completion.choices[0].message.content)\n",
        "\n",
        "    clean_str = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", completion.choices[0].message.content)\n",
        "    data = json.loads (clean_str)\n",
        "    texts = data[\"keywords\"]\n",
        "\n",
        "    return texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07920f33",
      "metadata": {},
      "source": [
        "**Re Ranker**  \n",
        "> Function to re-rank the set of texts based on semantic similarity with the query string  \n",
        "> since Key word search might result in jus text matched content, re-ranking is needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6179e8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def re_rank_text(query: str, ref_strings: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Re-rank reference strings based on cosine similarity to a query string.\n",
        "\n",
        "    Args:\n",
        "        query: The query text.\n",
        "        ref_strings: A list of reference texts to be re-ranked.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: The reference strings sorted by similarity (most similar first).\n",
        "    \"\"\"\n",
        "    _model = Embedder_1\n",
        "\n",
        "    if not ref_strings:\n",
        "        return []\n",
        "\n",
        "    # Encode query and references\n",
        "    query_emb = _model.encode(query, normalize_embeddings=True)\n",
        "    ref_embs = _model.encode(ref_strings, normalize_embeddings=True)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarities = util.cos_sim(query_emb, ref_embs)[0]  \n",
        "\n",
        "    # Sort reference strings by similarity (descending)\n",
        "    ranked_indices = similarities.argsort(descending=True)\n",
        "    ranked_refs = [ref_strings[i] for i in ranked_indices]\n",
        "\n",
        "    return ranked_refs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f68dcd9",
      "metadata": {},
      "source": [
        "**DB Connect**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849074f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to existing Vector DB and use data\n",
        "# Create a Lance DB Vector Base\n",
        "DB = lancedb.connect ('Vector_DB')\n",
        "\n",
        "# Create a Table and add the Chunks data\n",
        "table = DB.open_table (\"tech_ref\")\n",
        "print (table.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d99723ba",
      "metadata": {},
      "source": [
        "**Text Indexing**  \n",
        "Make Indexing based on the text field. This allows to run key word or text search in the DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e203bcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "table.create_fts_index(\"text\", replace=True)\n",
        "print (table.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe68401",
      "metadata": {},
      "source": [
        "#### Augmented Generation \n",
        "> With the Context information that is fetched from the knowledge repo, the Original query is sent to LLM for provinding answer  \n",
        "> It answers from within the context provided"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e33513",
      "metadata": {},
      "source": [
        "**Query Mechanism**  \n",
        "Pass the Original Query through the transformaton and expansion pipeline  \n",
        "Then consolidate before being used for retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b34d63ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query = \"How various industries taking advantage of AI?\"\n",
        "# Query = \"What is Private Cloud? Is it viable?\"\n",
        "# Query = \"How big is the IoT industry in terms of business volume?\"\n",
        "# Query = \"Hybrid CLoud and Edge are same?\"\n",
        "Query = \"How new trends of AI challenging Cloud Security?\"\n",
        "\n",
        "Context = []\n",
        "\n",
        "# First Make Semantic Search based on the Query\n",
        "Query_Vector = Embedder_2.encode (Query).tolist ()\n",
        "Results = table.search(Query_Vector).distance_type(\"cosine\").distance_range(upper_bound=0.6).limit(5).to_list ()\n",
        "\n",
        "# Append to the list\n",
        "Text_List = [r['text'] for r in Results]\n",
        "Context = Context + Text_List\n",
        "\n",
        "# Identtify Keywords from the query\n",
        "Keywords = query_keywords (Query)\n",
        "\n",
        "# Keyword search done in the Vector DB\n",
        "for word in Keywords :\n",
        "    \n",
        "    # Search is enabled by the Full Text Search Index in the DB\n",
        "    Results = table.search(word).limit(5).to_list ()\n",
        "    Text_List = [r['text'] for r in Results]\n",
        "    Context = Context + Text_List\n",
        "\n",
        "# Run a serch based on Query itself\n",
        "Results = table.search(Query).limit(5).to_list ()\n",
        "Text_List = [r['text'] for r in Results]\n",
        "Context = Context + Text_List\n",
        "\n",
        "# remove duplicates\n",
        "Context = list(set(Context))\n",
        "print (len(Context))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e941ff59",
      "metadata": {},
      "source": [
        "**Generation**  \n",
        "With the context build based on both meaning and word search, re-ranking is necessary  \n",
        "Once Re-ranked, take top_k and provide as context for generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d82685",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-rank the picked chunks of text based on semantic similarity with the original Query\n",
        "Re_Rank_Context = re_rank_text (Query, Context)\n",
        "Re_Rank_Context = Re_Rank_Context [:10]\n",
        "\n",
        "# Instruction for the LLM\n",
        "Instruction = \"\"\"You are given context information and a user query. You have to provide detailed answer to user query based on information provided in context.\n",
        "                Provide an informative answer to the user query **BASED** on the context.\n",
        "                If sufficient details are not in context, respond as \"No Sufficient Details\"\n",
        "            \"\"\"\n",
        "\n",
        "response = Gen_Client.models.generate_content(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                config =types.GenerateContentConfig(\n",
        "                            system_instruction=Instruction,\n",
        "                            # temperature=0.0\n",
        "                            ),\n",
        "                contents = [\"Context : \\n\"+str(Re_Rank_Context), \"User Query : \\n\"+Query]\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ed70c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-struture the output\n",
        "Instruction = \"\"\"You are given context information and a user query. You have to provide detailed answer (\"Answer\") to user query based on information provided in context.\n",
        "                Provide an informative answer to the user query **BASED** on the context.\n",
        "                If sufficient details are not in context, respond as \"No Sufficient Details\"\n",
        "                Also you will have to provide a summary of the context in 500 words (\"Context\"), as a justification to your answer\n",
        "                Respond text in JSON format with keys : {\"Answer\" : string, \"Context\" : Summary string}\n",
        "            \"\"\"\n",
        "\n",
        "response = Gen_Client.models.generate_content(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                config =types.GenerateContentConfig(\n",
        "                            system_instruction=Instruction,\n",
        "                            # temperature=0.0\n",
        "                            ),\n",
        "                contents = [\"Context : \\n\"+str(Re_Rank_Context), \"User Query : \\n\"+Query]\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc35e934",
      "metadata": {},
      "source": [
        "**More details**  \n",
        "Variation of response to include summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17e4f16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation with different LLM\n",
        "Instruction = \"\"\"You are given context information and a user query. You have to provide detailed answer to user query based on information provided in context.\n",
        "                Provide an informative answer to the user query **BASED** on the context. Provide as Marked Down Text\n",
        "                If sufficient details are not in context, respond as \"No Sufficient Details\"\n",
        "            \"\"\"\n",
        "\n",
        "messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": Instruction,\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Context : \\n\"+str(Re_Rank_Context)+\"\\n\\nUser Query : \\n\"+Query,\n",
        "            }\n",
        "        ]\n",
        "completion = Retrieval_Client.chat.completions.create(\n",
        "    messages=messages,    \n",
        "    # model=\"llama-3.3-70b-versatile\",\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Adv_RAG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
