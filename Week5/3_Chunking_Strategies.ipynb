{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Startegies\n",
    "> The typical 'chunking' considered ass splitting the reference contennt (knowledge base) into managable and meaningful pieces, before being vectorised  \n",
    "> If its done by fixed length, there may not be meaningful and logical content in each chunk  \n",
    "> There are specific chunking techniques, which would help the chunks (pieces) to hold logical and coherent content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevant Library Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitter functionality is provided by LangChain framework\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Make use of BS for hadling the web content\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "Functions that can be used in multiple places like a library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Main Content**  \n",
    "Given an URL, this function can get only the main content of the web page (leaving aside, side panel, navigation etc)  \n",
    "It can be feteched along with the html tag Or just as plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_content (url, type):\n",
    "\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove layout elements\n",
    "    for tag in soup([\"nav\", \"header\", \"footer\", \"aside\", \"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Check and get main section of the pages\n",
    "    main = soup.find(\"main\")\n",
    "\n",
    "    if not main:\n",
    "        \n",
    "        # fallback method, if no 'main' section in html page\n",
    "        candidates = soup.find_all(\"div\", recursive=True)\n",
    "        main = max(candidates, key=lambda c: len(c.get_text(strip=True)), default=soup.body)\n",
    "\n",
    "    # Get cleaned HTML content. Tags retained\n",
    "    main_html = str(main)\n",
    "\n",
    "    # If HTML content is required, provide with the tags\n",
    "    if type == 'html':\n",
    "        return (main_html)\n",
    "\n",
    "    # If text is requirred, provide only the text content\n",
    "    elif type == 'text':\n",
    "\n",
    "        text_soup = BeautifulSoup (main_html, \"html.parser\")\n",
    "        main_text = text_soup.get_text(separator=\"\\n\", strip=True)\n",
    "        return main_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence / Paragraph Chunking\n",
    "> This type is a strategy that splits the text based on natural split points of sentence, paragraph etc.  \n",
    "> Naturally the text are formulated into logical pieces that can be taken advantage to split  \n",
    "> it avoids chunk split by fixed length at unnatural points   \n",
    "> This will help retainning the meaningful content of  natural block of text into the chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitter based on Recursive charecter library\n",
    "\n",
    "# Define what are the splitters to be considered. There is default in library itself\n",
    "seperators = [\"\\n\\n\", \"\\n\", \".\"]\n",
    "\n",
    "# Splitter function based on seperator and the length criteria\n",
    "text_splitter = RecursiveCharacterTextSplitter (chunk_size=300, chunk_overlap=0,\n",
    "                                                length_function=len, is_separator_regex=False,\n",
    "                                                keep_separator=False,\n",
    "                                                separators=seperators,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence / Paragraph Chunking**  \n",
    "Get the main content of a web page in plain text  \n",
    "Further split the content into chunks based on length criteria and the natural seperators  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://cloud.google.com/learn/what-is-cloud-computing?hl=en\"\n",
    "url = \"https://www.ibm.com/think/topics/history-of-artificial-intelligence\"\n",
    "\n",
    "# Get the main content of the web page\n",
    "text_Content = get_main_content (url, \"text\")\n",
    "\n",
    "with open ('content.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "    print (text_Content, file=f)\n",
    "\n",
    "# Use the text splitter object to split the text based on the schema defined\n",
    "# Each chunk is a string element in a list\n",
    "docs = text_splitter.split_text(text_Content)\n",
    "\n",
    "with open ('chunks.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "    \n",
    "    for doc in docs :\n",
    "\n",
    "        print (doc, \"\\n---\")\n",
    "        print (doc, \"\\n---\", file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Aware Chunking\n",
    "> This type of chunking splits the content based on the document structure  \n",
    "> Normally documents are organised in terms of chapters with headings or sections  \n",
    "> The library provides methods to split the text based on document structure  \n",
    "> Having split based on the document structure, each chunk retains the complete and cohesive content that would be meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# levels of header tags in html to split on\n",
    "header_levels = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "# Define a Splitter object for HTML content from the lib\n",
    "# This library also gives splitter for Markdown, JSON etc\n",
    "html_splitter = HTMLHeaderTextSplitter(header_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.ibm.com/think/topics/history-of-artificial-intelligence\"\n",
    "\n",
    "HTML_Content = get_main_content (url, \"html\")\n",
    "\n",
    "docs = html_splitter.split_text (HTML_Content)\n",
    "\n",
    "with open ('chunks.txt', mode='w') as f:\n",
    "\n",
    "    for doc in docs :\n",
    "\n",
    "        try :\n",
    "\n",
    "            print (doc.metadata)\n",
    "            print (\"Content : \", doc.page_content,\"\\n---\")\n",
    "\n",
    "            print (\"Heading : \",doc.metadata,file=f)\n",
    "            print (\"Content : \", doc.page_content,\"\\n---\",file=f)\n",
    "\n",
    "        except Exception :\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Chunking\n",
    "> This relies on semantic (meaning) of the sentences and collate them based on similarity in meaning  \n",
    "> This brings a chunk to be meaningful part of the text, not just text splitter  \n",
    "> To do this, embedding models are used as a mechanism to identify the meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an embedding model\n",
    "Embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Prepare a Chunker, which provides various possibilites depending on the need\n",
    "Semantic_Splitter = SemanticChunker (Embedder, min_chunk_size=100, \n",
    "                                     buffer_size = 1, \n",
    "                                    #  breakpoint_threshold_amount = 85.0,\n",
    "                                    #  sentence_split_regex = '(?<=[.?!])\\\\s+|\\\\n+'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.ibm.com/think/topics/cloud-computing\"\n",
    "\n",
    "# Pull out main content in text format from an URL\n",
    "Text_Content = get_main_content (url, \"text\")\n",
    "\n",
    "with open ('content.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "    print (Text_Content, file=f)\n",
    "\n",
    "# Split as documents by the Chunker\n",
    "docs = Semantic_Splitter.create_documents([Text_Content])\n",
    "\n",
    "print (len(docs))\n",
    "with open ('chunks.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "\n",
    "    for doc in docs :\n",
    "\n",
    "        print (doc.page_content,\"\\n---\")\n",
    "        print (doc.page_content,\"\\n---\",file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example with plain text content itself\n",
    "with open ('AI_History.txt', mode='r', encoding=\"utf-8\") as f:\n",
    "    Text_Content = f.read ()\n",
    "\n",
    "docs = Semantic_Splitter.create_documents([Text_Content])\n",
    "\n",
    "print (len(docs))\n",
    "with open ('chunks.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "\n",
    "    for doc in docs :\n",
    "\n",
    "        print (doc.page_content,\"\\n---\")\n",
    "        print (doc.page_content,\"\\n---\",file=f)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Adv_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
