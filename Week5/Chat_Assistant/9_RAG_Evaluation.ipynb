{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a813f51",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Assistant import Retrieve_Context, Ask_Assistant\n",
    "\n",
    "import lancedb\n",
    "#import openai\n",
    "from dotenv import load_dotenv\n",
    "import groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e32a0",
   "metadata": {},
   "source": [
    "#### Using LLM as a Judge  \n",
    "> For evaluation of the RAG response, an effective method is to use another LLM instance to evaluate the response provided  \n",
    "> This can be made by evaluating the RAG response with the criteria such as Groundedness, Completeness, Relevance etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b763a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise an client object with API key\n",
    "load_dotenv ()\n",
    "Eval_Client = groq.Groq ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Lance DB Vector Base\n",
    "DB = lancedb.connect ('Quick_Ref')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1362927",
   "metadata": {},
   "source": [
    "**Sample Questions**  \n",
    "Prepare a list of sample questions based on the subject that is preserved in the Knowledge base  \n",
    "This list will be used to generate Response from the RAG pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_Name = 'temp'\n",
    "\n",
    "Queries = [\"How different the software development be in Quantum computers?\",\n",
    "           \"What is the processor in Quantum Computer?\",\n",
    "           \"Better computing performance\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add98198",
   "metadata": {},
   "source": [
    "**Evaluation**  \n",
    "> For a set of questions, generate RAG response  \n",
    "> For the same set of questions, retrieve the content using the same retrieval mechanism (consistency)  \n",
    "> Based on the question, context and generated answer, use another LLM to make evaluation on the required criteria  \n",
    "> The criteria can also talk about generic qualititative aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ac5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Judge_Instruction = \"\"\" You are an evaluator who can check an answer provided by LLM.\n",
    "                    You are given with Context information, User Query and LLM Answer.\n",
    "                    You will have to evalute the answer with following \"criteria\".\n",
    "                    1. \"Groundedness\" : Whether the answer is based on details in context\n",
    "                    2. \"Completeness\" : Whether the answer is complete to answer the question\n",
    "                    3. \"Relevance\" : Whether the answer is relevant to the question\n",
    "\n",
    "                    Provide your evaluation in (Good / Moderate / Bad) scale in format. \n",
    "                    [{\"Criteria\" : Evaluation}]\n",
    "                    \"\"\"\n",
    "\n",
    "for Query in Queries :\n",
    "\n",
    "    # Fetch the context, with same method as it is given for Generation\n",
    "    Context = Retrieve_Context (Query, 'temp')\n",
    "\n",
    "    # Get the answer from the Generation Assistant\n",
    "    Answer = Ask_Assistant (\"\", Query=Query, table_name='temp')\n",
    "\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": Judge_Instruction,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Context : \\n\"+str(Context),\n",
    "    },    \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"User Query : \\n\"+Query,\n",
    "    },    \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"LLM Answer : \\n\"+Answer,\n",
    "    }, \n",
    "    ]\n",
    "\n",
    "    completion = Eval_Client.chat.completions.create(\n",
    "        messages=messages,    \n",
    "        # model=\"llama-3.3-70b-versatile\",\n",
    "        model=\"openai/gpt-oss-120b\",\n",
    "        stop=None,\n",
    "    )\n",
    "    print (\"Question :\\n\", Query, \"Answer : \\n\",Answer, \"---\\n\",completion.choices[0].message.content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Adv_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
