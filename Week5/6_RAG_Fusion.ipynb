{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# from tqdm import tqdm\n",
        "import lancedb\n",
        "import openai\n",
        "import groq\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "from google import genai\n",
        "from google.genai import types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inputs\n",
        "LANCE_DB_NAME = 'Vector_DB'\n",
        "LANCEDB_TABLE_NAME = 'tech_ref'\n",
        "TOP_K_PER_QUERY = 5\n",
        "MAX_COMBINED_RESULTS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Embedder_1 = SentenceTransformer (\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Embedder_2 = SentenceTransformer (\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "\n",
        "# Initialise an client object with API key\n",
        "load_dotenv ()\n",
        "Retrieval_Client = groq.Groq ()\n",
        "Gen_Client = genai.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e1876c",
      "metadata": {},
      "source": [
        "#### Utlities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "220924c2",
      "metadata": {},
      "source": [
        "**Query tranformation**  \n",
        "> Function to transform the query into more precide and paraphrased ones  \n",
        "> This might help to retireve information that are hidden in details  \n",
        "> By paraphrasing with the help of LLM, the query is brought with more nuances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query transformation (LLM + fallback)\n",
        "\n",
        "def transform_query (query: str, n_paraphrases: int = 3) -> List[str]:\n",
        "    prompt = (\n",
        "        'You are given a user query. With that, produce:\\n'\n",
        "        f'1) a precise reformulation suitable for content retrieval (\"precise\")\\n'\n",
        "        f'2) {n_paraphrases} concise paraphrases of the original query suitable for semantic retrieval (\"paraphrases\")\\n'\n",
        "        'Return JSON with keys: \"precise\" (string) and \"paraphrases\" (list of strings).\\n'\n",
        "        'User query: ' + query\n",
        "    )\n",
        "\n",
        "    messages=[\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt,\n",
        "    }\n",
        "    ]\n",
        "    completion = Retrieval_Client.chat.completions.create(\n",
        "        messages=messages,    \n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        # model=\"openai/gpt-oss-120b\",\n",
        "        stop=None,\n",
        "    )\n",
        "\n",
        "    # print (completion.choices[0].message.content)\n",
        "\n",
        "    clean_str = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", completion.choices[0].message.content)\n",
        "    data = json.loads (clean_str)\n",
        "    texts = [data[\"precise\"]] + data[\"paraphrases\"]\n",
        "\n",
        "    return texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f39032e1",
      "metadata": {},
      "source": [
        "**Query Expansion**  \n",
        "> Function to Expand the query into multiple queries with the same intent of original query  \n",
        "> This would help to retireve information that are brought out by variations\n",
        "> By creating varations with the help of LLM, query is possible to get into more semantically similar areas while retrieving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf255a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def expand_query (query: str, n_alternates: int = 3) -> List[str]:\n",
        "    prompt = (\n",
        "        f'Generate {n_alternates} diverse query variations that can expand search horizon, but preserve intent of the following user query:\\nQuery: {query}\\n'\n",
        "        'Return JSON with keys: \"alternates\" (list of strings).\\n'\n",
        "        'User query: ' + query\n",
        "    )\n",
        "\n",
        "    messages=[\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt,\n",
        "    }\n",
        "    ]\n",
        "    completion = Retrieval_Client.chat.completions.create(\n",
        "        messages=messages,    \n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        # model=\"openai/gpt-oss-120b\",\n",
        "        stop=None,\n",
        "    )\n",
        "\n",
        "    # print (completion.choices[0].message.content)\n",
        "\n",
        "    clean_str = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", completion.choices[0].message.content)\n",
        "    data = json.loads (clean_str)\n",
        "    texts = data[\"alternates\"]\n",
        "\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849074f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to existing Vector DB and use data\n",
        "# Create a Lance DB Vector Base\n",
        "DB = lancedb.connect ('Vector_DB')\n",
        "\n",
        "# Create a Table and add the Chunks data\n",
        "table = DB.open_table (\"tech_ref\")\n",
        "print (table.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e33513",
      "metadata": {},
      "source": [
        "**Query Mechanism**  \n",
        "Pass the Original Query through the transformaton and expansion pipeline  \n",
        "Then consolidate before being used for retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b34d63ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query = \"How is the industry adapting to AI?\"\n",
        "# Query = \"Cloud computing Vs Edge computing\"\n",
        "Query = \"What is the timeline it took from ML to AI?\"\n",
        "# Query = \"Tell me what was the motivation for the indutry to develop Edge Technology\"\n",
        "# Query = \"Did it rain yesterday during the business hours?\"\n",
        "\n",
        "trans_queries = transform_query (Query)\n",
        "expand_queries = expand_query (Query)\n",
        "\n",
        "# print (\"Transformed Queries : \\n\")\n",
        "# for t in trans_queries:\n",
        "#     print (t)\n",
        "\n",
        "# print (\"Expanded Queries : \\n\")\n",
        "# for e in expand_queries:\n",
        "#     print (e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c540a3f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search from the Vector DB for each of the queries that are formulated\n",
        "queries  = trans_queries + expand_queries\n",
        "\n",
        "Context = []\n",
        "\n",
        "for query in queries :\n",
        "\n",
        "    Query_Vector = Embedder_2.encode (query).tolist ()\n",
        "    Results = table.search(Query_Vector).distance_type(\"cosine\").distance_range(upper_bound=0.6).limit(5).to_list ()\n",
        "\n",
        "    # print (len (Results))\n",
        "\n",
        "    Text_List = [r['text'] for r in Results]\n",
        "    Context = Context + Text_List\n",
        "\n",
        "Context = list(set(Context))\n",
        "print (len(Context))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df61ef85",
      "metadata": {},
      "source": [
        "#### Augmented Generation \n",
        "> With the Context information that is fetched from the knowledge repo, the Original query is sent to LLM for provinding answer  \n",
        "> It answers from within the context provided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c3ebb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instruction for the LLM\n",
        "Instruction = \"\"\"You are given context information and a user query. You have to provide detailed answer to user query based on information provided in context.\n",
        "                Provide an informative answer to the user query **BASED** on the context.\n",
        "                If sufficient details are not in context, respond as \"No Sufficient Details\"\n",
        "            \"\"\"\n",
        "\n",
        "response = Gen_Client.models.generate_content(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                config =types.GenerateContentConfig(\n",
        "                            system_instruction=Instruction,\n",
        "                            # temperature=0.0\n",
        "                            ),\n",
        "                contents = [\"Context : \\n\"+str(Context), \"User Query : \\n\"+Query]\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc35e934",
      "metadata": {},
      "source": [
        "**More details**  \n",
        "Variation of response to include summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ed70c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-struture the output\n",
        "Instruction = \"\"\"You are given context information and a user query. You have to provide detailed answer (\"Answer\") to user query based on information provided in context.\n",
        "                Provide an informative answer to the user query **BASED** on the context.\n",
        "                If sufficient details are not in context, respond as \"No Sufficient Details\"\n",
        "                Also you will have to provide a summary of the context in 300 words (\"Context\")\n",
        "                Respond in JSON format with keys : {\"Answer\" : string, \"Context\" : Summary string}\n",
        "            \"\"\"\n",
        "\n",
        "response = Gen_Client.models.generate_content(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                config =types.GenerateContentConfig(\n",
        "                            system_instruction=Instruction,\n",
        "                            # temperature=0.0\n",
        "                            ),\n",
        "                contents = [\"Context : \\n\"+str(Context), \"User Query : \\n\"+Query]\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17e4f16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation with different LLM\n",
        "Instruction = \"\"\"You are given context information and a user query. You have to provide detailed answer to user query based on information provided in context.\n",
        "                Provide an informative answer to the user query **BASED** on the context.\n",
        "                If sufficient details are not in context, respond as \"No Sufficient Details\"\n",
        "            \"\"\"\n",
        "\n",
        "messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": Instruction,\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Context : \\n\"+str(Context)+\"\\n\\nUser Query : \\n\"+Query,\n",
        "            }\n",
        "        ]\n",
        "completion = Retrieval_Client.chat.completions.create(\n",
        "    messages=messages,    \n",
        "    # model=\"llama-3.3-70b-versatile\",\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Adv_RAG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
