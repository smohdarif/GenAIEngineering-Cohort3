{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Data Retrieval \n",
    "> Meta data can be used effectively in retrieval of data from knowledge Base  \n",
    "> It can provide another aspect of search possibility to pull out relevant information  \n",
    "> Further the semantic search and meta data retrieval can be used in Hybrid mode to improve the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "# Sentence transformers to use the embedding models locally\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Import required class from Google\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialise an client object with API key\n",
    "load_dotenv ()\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Models**  \n",
    "2 differenet models are used for embedding  \n",
    "The embedding that is needed for vector search shall be teh same model that is used for vector DB creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedder_1 = SentenceTransformer (\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "Embedder_2 = SentenceTransformer (\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility**  \n",
    "> Function to match and get the relevant meta data from the entire meta data present in vector Db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Match_Meta_Data (query, meta_data, meta_data_emb, top_k=2):\n",
    "\n",
    "    # Query    \n",
    "    query_emb = Embedder_1.encode(query, normalize_embeddings=True)\n",
    "\n",
    "    # Top-k semantic search\n",
    "    hits = util.semantic_search (query_embeddings=query_emb, corpus_embeddings=meta_data_emb, top_k=top_k)\n",
    "\n",
    "    Matches = []\n",
    "    for hit in hits[0]:\n",
    "\n",
    "        Matches = Matches + [meta_data[hit['corpus_id']]]\n",
    "    \n",
    "    return Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2 Different Embedding model from Sentence Transformer  \n",
    "> One used for Search from Vector DB (Vectors in DB and query vector shall be with same embedding model)  \n",
    "> the other one used for handling meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Connect to the Database that was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to existing Vector DB and use data\n",
    "# Create a Lance DB Vector Base\n",
    "DB = lancedb.connect ('Vector_DB')\n",
    "\n",
    "# Create a Table and add the Chunks data\n",
    "table = DB.open_table (\"tech_ref\")\n",
    "print (table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query a vector\n",
    "# Query = \"There are many service providers\"\n",
    "# Query = \"Where are the servers located?\"\n",
    "Query = \"What shall be the deciding factor for my embedded system?\"\n",
    "\n",
    "Query_Vector = Embedder_2.encode (Query).tolist ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different Search**  \n",
    "The search with similarity and top_k always returns a result. This is because criteria is only about top_k  \n",
    "Whereas setting a distance threshold can identify really meaningful matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nSimilarity : Top k :\")\n",
    "Results = table.search(Query_Vector).distance_type(\"cosine\").limit(5).to_list ()\n",
    "\n",
    "for Rs in Results :\n",
    "\n",
    "    print (Rs['_distance'],Rs['source'],\" ## \",Rs ['text'])\n",
    "\n",
    "print (\"\\nSimilarity : Distance threshold then Top k :\")\n",
    "Results = table.search(Query_Vector).distance_type(\"cosine\").distance_range(upper_bound=0.6).limit(5).to_list ()\n",
    "\n",
    "for Rs in Results :\n",
    "\n",
    "    print (Rs['_distance'],Rs['source'],\" ## \",Rs ['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Meta Data filtering**  \n",
    "> The meta data fields present in th vector DB can be used to pre filter the content. Since the meta data provides the broad meaning of the content, it can be a good reference to narrow down  \n",
    "> One possibility of meta data filtering is to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the sources and topics.\n",
    "DF = table.to_pandas ()\n",
    "Sources = DF['source'].unique ().tolist ()\n",
    "Topics = DF['topic'].unique ().tolist ()\n",
    "Topics_Emb = Embedder_1.encode(Topics, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Formulate the search query that can be used in vector DB filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = Match_Meta_Data (Query, Topics, Topics_Emb, 3)\n",
    "filter_text = \"(topic IN (\" + (\",\".join(f\"'{x}'\" for x in filter)) + \"))\"\n",
    "filter_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter + Search**  \n",
    "> Filtering criteria applied. Based on the outcome (the chunks that are filtered out), then semantic search is applied  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\n Meta Data Filtered : Top k :\")\n",
    "\n",
    "Results = table.search(Query_Vector).where (filter_text).distance_type(\"cosine\").limit(5).to_list ()\n",
    "\n",
    "for Rs in Results :\n",
    "\n",
    "    print (Rs['_distance'],Rs['source'],\" ## \",Rs ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation + Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Meta data Overview**  \n",
    "Use LLM to create a summary of the content present in the meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction for the LLM\n",
    "Instruction = \"\"\"You will be provided a list of topics. Make a summary what is the being discussed, based on the list of topics.\n",
    "                 Summary in 100 words. Just provide summary text. **No additional Text**\n",
    "            \"\"\"\n",
    "Topic_List  = \"Topics : \\n\"+str(Topics)\n",
    "\n",
    "   \n",
    "response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                config =types.GenerateContentConfig(\n",
    "                            system_instruction=Instruction,\n",
    "                            # temperature=0.0\n",
    "                            ),\n",
    "                contents =Topic_List\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Context**  \n",
    "Find relevant information to the query from Vector DB. Search is done in multiple methods and results consolidated  \n",
    "Then its made into context information for the LLM to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query = \"What is Cloud Computing?\"\n",
    "# Query = \"How does IoT get benefitted by Edge Technology?\"\n",
    "# Query = \"Is GPU Mandatory for AI?\"\n",
    "Query = \"How can I estimate the Cloud infra cost for my project?\"\n",
    "# Query = \"Is Cloud computing cost effective?\"\n",
    "\n",
    "filter = Match_Meta_Data (Query, Topics, Topics_Emb, 3)\n",
    "filter_text = \"(topic IN (\" + (\",\".join(f\"'{x}'\" for x in filter)) + \"))\"\n",
    "\n",
    "Query_Vector = Embedder_2.encode (Query).tolist ()\n",
    "\n",
    "# Typical Similarity Seach with threshold\n",
    "Results_1 = table.search(Query_Vector).distance_type(\"cosine\").distance_range(upper_bound=0.6).limit(5).to_list ()\n",
    "print (len(Results_1))\n",
    "\n",
    "# Search with meta data filtering\n",
    "Results_2 = table.search(Query_Vector).where (filter_text).distance_type(\"cosine\").limit(5).to_list ()\n",
    "print (len(Results_2))\n",
    "\n",
    "# Remove Duplicates\n",
    "Context = [d['text'] for d in Results_1]\n",
    "Context = Context + [d['text'] for d in Results_2]\n",
    "\n",
    "Context = list(set(Context))\n",
    "print (len(Context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction for the LLM\n",
    "Instruction = \"\"\"You will be given context information and a user query. You have to provide an answer to user query based on information provided in context.\n",
    "                Answer **ONLY** based on context. If sufficient details are not in context, respond as \"No Sufficient Details\"\n",
    "            \"\"\"\n",
    "   \n",
    "response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                config =types.GenerateContentConfig(\n",
    "                            system_instruction=Instruction,\n",
    "                            # temperature=0.0\n",
    "                            ),\n",
    "                contents = [\"Context : \\n\"+str(Context), \"User Query : \\n\"+Query]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                config =types.GenerateContentConfig(\n",
    "                            system_instruction=\"Respond to User query\",\n",
    "                            ),\n",
    "                contents = \"User Query : \\n\"+Query\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Adv_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
