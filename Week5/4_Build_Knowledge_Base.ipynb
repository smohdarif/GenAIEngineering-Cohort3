{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitter functionality is provided by LangChain framework\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Make use of BS for hadling the web content\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import lancedb\n",
    "\n",
    "# Sentence transformers to use the embedding models locally\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "\n",
    "> Library functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Meta Data**  \n",
    "From the parsed and split content, this function helps to create meta data in a custom way, that can be used while creating Knowledge DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_data_from_headings (heading: dict, n: int = 1, from_end: bool = True, sep: str = \" : \") -> str:\n",
    "    \"\"\"\n",
    "    Concatenates n values from a heading dictionary, either from the start or from the end.\n",
    "\n",
    "    Param:\n",
    "        heading (dict): Input dictionary for headings.\n",
    "        n (int): Number of elements to take.\n",
    "        from_end (bool): If True, take from the end; else from the start.\n",
    "        sep (str): Optional separator to use between concatenated strings.\n",
    "\n",
    "    Returns: Meta data as concatenation of headings.\n",
    "    \"\"\"\n",
    "    values = list(heading.values())\n",
    "\n",
    "    if n <= 0:\n",
    "        n = 1\n",
    "    if n > len(values):\n",
    "        n = len(values)\n",
    "\n",
    "    # Select n items from start or end\n",
    "    selected = values[-n:] if from_end else values[:n]\n",
    "\n",
    "    # Always concatenate in forward direction\n",
    "    return sep.join(str(v) for v in selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Main Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_content (url, type):\n",
    "\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove layout elements\n",
    "    for tag in soup([\"nav\", \"header\", \"footer\", \"aside\", \"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Check and get main section of the pages\n",
    "    main = soup.find(\"main\")\n",
    "\n",
    "    if not main:\n",
    "        \n",
    "        # fallback method, if no 'main' section in html page\n",
    "        candidates = soup.find_all(\"div\", recursive=True)\n",
    "        main = max(candidates, key=lambda c: len(c.get_text(strip=True)), default=soup.body)\n",
    "\n",
    "    # Get cleaned HTML content. Tags retained\n",
    "    main_html = str(main)\n",
    "\n",
    "    # If HTML content is required, provide with the tags\n",
    "    if type == 'html':\n",
    "        return (main_html)\n",
    "\n",
    "    # If text is requirred, provide only the text content\n",
    "    elif type == 'text':\n",
    "\n",
    "        text_soup = BeautifulSoup (main_html, \"html.parser\")\n",
    "        main_text = text_soup.get_text(separator=\"\\n\", strip=True)\n",
    "        return main_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Pass Chunking**\n",
    "> Often the scenario could be to incorporate multiple ways of chunking to have better granularity and meaning in the chunks  \n",
    "> The Sentence and content aware chunkers are used in Combination to retain the context and be granular as well  \n",
    "> The context is captured by the Meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what are the splitters to be considered. There is default in library itself\n",
    "seperators = [\".\", \"?\", \"!\"]\n",
    "\n",
    "# Splitter function based on seperator and the length criteria\n",
    "text_splitter = RecursiveCharacterTextSplitter (chunk_size=300, chunk_overlap=0,\n",
    "                                                length_function=len, is_separator_regex=False,\n",
    "                                                keep_separator=False,\n",
    "                                                separators=seperators,\n",
    "                                                )\n",
    "\n",
    "# levels of header tags in html to split on\n",
    "header_levels = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "# Define a Splitter object for HTML content from the lib\n",
    "# This library also gives splitter for Markdown, JSON etc\n",
    "html_splitter = HTMLHeaderTextSplitter(header_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine 2 methods**  \n",
    "Get the content and split it based on document structure first  \n",
    "Some of the chunks can be big, because of the way the text is present  \n",
    "Pass those blocks for one more level of splitting by sentences  \n",
    "Capture the meta data from the headings and use it along with the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Chunks (url, source, chunk_size_limit):\n",
    "\n",
    "    # Get the main content\n",
    "    HTML_Content = get_main_content (url, \"html\")\n",
    "\n",
    "    # Chunk based on document structure\n",
    "    docs = html_splitter.split_text (HTML_Content)\n",
    "\n",
    "    # Start with empty list\n",
    "    Chunks = []\n",
    "\n",
    "    with open ('chunks.txt', mode='w') as f:\n",
    "\n",
    "        for doc in docs :\n",
    "\n",
    "            try :\n",
    "\n",
    "                meta_data = meta_data_from_headings (doc.metadata)\n",
    "\n",
    "                if not meta_data:\n",
    "                    meta_data = 'Generic'\n",
    "\n",
    "                # If the chunk is too long,\n",
    "                if (len(doc.page_content) > chunk_size_limit):\n",
    "\n",
    "                    # Split by sentece(s) by shorter lenth\n",
    "                    splits = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "                    # Make them individual chunk with same meta data\n",
    "                    for split in splits:\n",
    "\n",
    "                        # Capture if the meta data and text are not the same\n",
    "                        if (meta_data != split):\n",
    "\n",
    "                            Chunk = {'source': source,'topic' : meta_data, 'text' : split}\n",
    "                            print (Chunk, \"\\n----\",file=f)\n",
    "\n",
    "                            Chunks = Chunks + [Chunk]\n",
    "                        \n",
    "                else :\n",
    "                    \n",
    "                    if (meta_data != doc.page_content):\n",
    "                        \n",
    "                        Chunk = {'source': source, 'topic' : meta_data, 'text' : doc.page_content}\n",
    "                        print (Chunk, \"\\n----\",file=f)\n",
    "                        Chunks = Chunks + [Chunk]\n",
    "                \n",
    "                # print (doc.metadata)\n",
    "                # print (\"Content : \", doc.page_content,\"\\n---\")\n",
    "                \n",
    "            except Exception :\n",
    "                pass\n",
    "\n",
    "    print (len(Chunks))\n",
    "\n",
    "    return Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ibm.com/think/topics/cloud-computing\"\n",
    "\n",
    "Chunks = Build_Chunks (url, \"IBM\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorise the data**  \n",
    "> Once the chunks are creared along with the supporging data, use embedding model and craete vectors  \n",
    "> Use a HF model which is suitable for general purpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedder = SentenceTransformer (\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors and store in the Chunks \n",
    "for idx, Chunk in enumerate (Chunks):\n",
    "\n",
    "    vector = Embedder.encode (Chunk['text'])\n",
    "    # print (type(vector))\n",
    "    # print (vector)\n",
    "\n",
    "    Chunks[idx]['vector'] = vector.tolist ()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check various topics existing in all Chunks\n",
    "Topics = list({c[\"topic\"] for c in Chunks})\n",
    "Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Lance DB Vector Base\n",
    "DB = lancedb.connect ('Vector_DB')\n",
    "\n",
    "# Create a Table and add the Chunks data\n",
    "table = DB.create_table(\"article\", data=Chunks, mode=\"overwrite\") \n",
    "print (table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query a vector\n",
    "Query = \"Platforms used as business in today's world\"\n",
    "\n",
    "Query_Vector = Embedder.encode (Query).tolist ()\n",
    "\n",
    "Results = table.search(Query_Vector).limit(5).to_list ()\n",
    "\n",
    "for Rs in Results :\n",
    "\n",
    "    print (Rs['_distance'],\" ## \",Rs ['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Tech Repo**  \n",
    "> From various sources in internet, create a knowledge repo with all information chunked and vectorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather multiple reference material for Technology information\n",
    "References = [{'Source' : 'Microsoft', 'url' : \"https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-cloud-computing\"},\n",
    "              {'Source' : 'IBM', 'url' : \"https://www.ibm.com/think/topics/cloud-computing\"},\n",
    "              {'Source' : 'Oracle', 'url' : \"https://www.oracle.com/in/cloud/what-is-cloud-computing/\"},\n",
    "              {'Source' : 'AWS', 'url' : \"https://aws.amazon.com/what-is/iot/\"},\n",
    "              {'Source' : 'IBM', 'url' : \"https://www.ibm.com/think/topics/edge-ai\"},              \n",
    "              {'Source' : 'Microsoft', 'url' : \"https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-edge-computing\"},\n",
    "              {'Source' : 'IBM', 'url' : \"https://www.ibm.com/think/topics/edge-computing\"},              \n",
    "              {'Source' : 'Fortinet', 'url' : \"https://www.fortinet.com/resources/cyberglossary/edge-computing\"},\n",
    "              {'Source' : 'NVIDIA', 'url' : \"https://blogs.nvidia.com/blog/what-is-edge-ai/\"},\n",
    "              {'Source' : 'MIT', 'url' : \"https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained\"},\n",
    "              {'Source' : 'AWS', 'url' : \"https://aws.amazon.com/what-is/machine-learning/\"}\n",
    "            ]\n",
    "\n",
    "Chunks = []\n",
    "for Ref in References:\n",
    "    \n",
    "    Parts = Build_Chunks (Ref['url'], Ref ['Source'], 500)\n",
    "    Chunks = Chunks + Parts\n",
    "\n",
    "print (len(Chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedder_1 = SentenceTransformer (\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors and store in the Chunks \n",
    "for idx, Chunk in enumerate (Chunks):\n",
    "\n",
    "    vector = Embedder_1.encode (Chunk['text'])\n",
    "    Chunks[idx]['vector'] = vector.tolist ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Table and add the Chunks data\n",
    "table = DB.create_table(\"tech_ref\", data=Chunks, mode=\"overwrite\") \n",
    "print (table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query a vector\n",
    "Query = \"There are many service providers\"\n",
    "\n",
    "Query_Vector = Embedder_1.encode (Query).tolist ()\n",
    "\n",
    "# Results = table.search(Query_Vector).distance_type(\"cosine\").limit(5).to_list ()\n",
    "Results = table.search().where(\"topic IN ('What is edge computing?')\").to_list ()\n",
    "\n",
    "for Rs in Results :\n",
    "\n",
    "    # print (Rs['_distance'],Rs['source'],\" ## \",Rs ['text'])\n",
    "    print (Rs['source'],\" ## \",Rs ['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Adv_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
