{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitter functionality is provided by LangChain framework\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Make use of BS for hadling the web content\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import lancedb\n",
    "\n",
    "# Sentence transformers to use the embedding models locally\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "\n",
    "> Library functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Meta Data**  \n",
    "From the parsed and split content, this function helps to create meta data in a custom way, that can be used while creating Knowledge DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_data_from_headings (heading: dict, n: int = 1, from_end: bool = True, sep: str = \" : \") -> str:\n",
    "    \"\"\"\n",
    "    Concatenates n values from a heading dictionary, either from the start or from the end.\n",
    "\n",
    "    Param:\n",
    "        heading (dict): Input dictionary for headings.\n",
    "        n (int): Number of elements to take.\n",
    "        from_end (bool): If True, take from the end; else from the start.\n",
    "        sep (str): Optional separator to use between concatenated strings.\n",
    "\n",
    "    Returns: Meta data as concatenation of headings.\n",
    "    \"\"\"\n",
    "    values = list(heading.values())\n",
    "\n",
    "    if n <= 0:\n",
    "        n = 1\n",
    "    if n > len(values):\n",
    "        n = len(values)\n",
    "\n",
    "    # Select n items from start or end\n",
    "    selected = values[-n:] if from_end else values[:n]\n",
    "\n",
    "    # Always concatenate in forward direction\n",
    "    return sep.join(str(v) for v in selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Main Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_content (url, type):\n",
    "\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove layout elements\n",
    "    for tag in soup([\"nav\", \"header\", \"footer\", \"aside\", \"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Check and get main section of the pages\n",
    "    main = soup.find(\"main\")\n",
    "\n",
    "    if not main:\n",
    "        \n",
    "        # fallback method, if no 'main' section in html page\n",
    "        candidates = soup.find_all(\"div\", recursive=True)\n",
    "        main = max(candidates, key=lambda c: len(c.get_text(strip=True)), default=soup.body)\n",
    "\n",
    "    # Get cleaned HTML content. Tags retained\n",
    "    main_html = str(main)\n",
    "\n",
    "    # If HTML content is required, provide with the tags\n",
    "    if type == 'html':\n",
    "        return (main_html)\n",
    "\n",
    "    # If text is requirred, provide only the text content\n",
    "    elif type == 'text':\n",
    "\n",
    "        text_soup = BeautifulSoup (main_html, \"html.parser\")\n",
    "        main_text = text_soup.get_text(separator=\"\\n\", strip=True)\n",
    "        return main_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Pass Chunking**\n",
    "> Often the scenario could be to incorporate multiple ways of chunking to have better granularity and meaning in the chunks  \n",
    "> The Sentence and content aware chunkers are used in Combination to retain the context and be granular as well  \n",
    "> The context is captured by the Meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what are the splitters to be considered. There is default in library itself\n",
    "seperators = [\".\", \"?\", \"!\"]\n",
    "\n",
    "# Splitter function based on seperator and the length criteria\n",
    "text_splitter = RecursiveCharacterTextSplitter (chunk_size=300, chunk_overlap=0,\n",
    "                                                length_function=len, is_separator_regex=False,\n",
    "                                                keep_separator=False,\n",
    "                                                separators=seperators,\n",
    "                                                )\n",
    "\n",
    "# levels of header tags in html to split on\n",
    "header_levels = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "# Define a Splitter object for HTML content from the lib\n",
    "# This library also gives splitter for Markdown, JSON etc\n",
    "html_splitter = HTMLHeaderTextSplitter(header_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine 2 methods**  \n",
    "Get the content and split it based on document structure first  \n",
    "Some of the chunks can be big, because of the way the text is present  \n",
    "Pass those blocks for one more level of splitting by sentences  \n",
    "Capture the meta data from the headings and use it along with the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Chunks (url, source, chunk_size_limit):\n",
    "\n",
    "    # Get the main content\n",
    "    HTML_Content = get_main_content (url, \"html\")\n",
    "\n",
    "    # Chunk based on document structure\n",
    "    docs = html_splitter.split_text (HTML_Content)\n",
    "\n",
    "    # Start with empty list\n",
    "    Chunks = []\n",
    "\n",
    "    with open ('chunks.txt', mode='w') as f:\n",
    "\n",
    "        for doc in docs :\n",
    "\n",
    "            try :\n",
    "\n",
    "                meta_data = meta_data_from_headings (doc.metadata)\n",
    "\n",
    "                if not meta_data:\n",
    "                    meta_data = 'Generic'\n",
    "\n",
    "                # If the chunk is too long,\n",
    "                if (len(doc.page_content) > chunk_size_limit):\n",
    "\n",
    "                    # Split by sentece(s) by shorter lenth\n",
    "                    splits = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "                    # Make them individual chunk with same meta data\n",
    "                    for split in splits:\n",
    "\n",
    "                        # Capture if the meta data and text are not the same\n",
    "                        if (meta_data != split):\n",
    "\n",
    "                            Chunk = {'source': source,'topic' : meta_data, 'text' : split}\n",
    "                            print (Chunk, \"\\n----\",file=f)\n",
    "\n",
    "                            Chunks = Chunks + [Chunk]\n",
    "                        \n",
    "                else :\n",
    "                    \n",
    "                    if (meta_data != doc.page_content):\n",
    "                        \n",
    "                        Chunk = {'source': source, 'topic' : meta_data, 'text' : doc.page_content}\n",
    "                        print (Chunk, \"\\n----\",file=f)\n",
    "                        Chunks = Chunks + [Chunk]\n",
    "                \n",
    "                # print (doc.metadata)\n",
    "                # print (\"Content : \", doc.page_content,\"\\n---\")\n",
    "                \n",
    "            except Exception :\n",
    "                pass\n",
    "\n",
    "    print (len(Chunks))\n",
    "\n",
    "    return Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.ibm.com/think/topics/cloud-computing\"\n",
    "\n",
    "Chunks = Build_Chunks (url, \"IBM\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorise the data**  \n",
    "> Once the chunks are creared along with the supporging data, use embedding model and craete vectors  \n",
    "> Use a HF model which is suitable for general purpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedder = SentenceTransformer (\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors and store in the Chunks \n",
    "for idx, Chunk in enumerate (Chunks):\n",
    "\n",
    "    vector = Embedder.encode (Chunk['text'])\n",
    "    # print (type(vector))\n",
    "    # print (vector)\n",
    "\n",
    "    Chunks[idx]['vector'] = vector.tolist ()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check various topics existing in all Chunks\n",
    "Topics = list({c[\"topic\"] for c in Chunks})\n",
    "Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Lance DB Vector Base\n",
    "DB = lancedb.connect ('Vector_DB')\n",
    "\n",
    "# Create a Table and add the Chunks data\n",
    "table = DB.create_table(\"article\", data=Chunks, mode=\"overwrite\") \n",
    "print (table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1794710159301758  ##  Platform as a service (PaaS)  \n",
      "With PaaS the cloud provider hosts everything at their data center. These include servers, networks, storage, operating system software, and databases\n",
      "1.2251911163330078  ##  business sustainability  \n",
      "All major cloud players have made net-zero commitments to reduce their carbon footprints and help clients reduce the energy they typically consume using an on-premises setup. For instance, IBM is driven by initiatives to reach NetZero by 2030.  \n",
      "sustainable procurement\n",
      "1.2362244129180908  ##  For example, an organization can choose AWS for its global reach with web hosting, IBM Cloud for data analytics and platforms and Microsoft Azure for its security features\n",
      "1.264312982559204  ##  provides software developers with an on-demand platform—hardware, complete software stack, infrastructure and development tools—for running, developing and managing applications without the cost, complexity and inflexibility of maintaining that platform on-premises\n",
      "1.2669360637664795  ##  Increased speed and agility  \n",
      "With cloud technologies, your organization can use enterprise applications in minutes instead of waiting weeks or months for IT to respond to a request, purchase and configure supporting hardware and install software\n"
     ]
    }
   ],
   "source": [
    "# Query a vector\n",
    "# Make sure to open the correct table and use matching embedder\n",
    "DB = lancedb.connect('Vector_DB')\n",
    "table = DB.open_table(\"article\")  # Open the article table explicitly\n",
    "\n",
    "Query = \"Platforms used as business in today's world\"\n",
    "\n",
    "# Check table schema to determine which embedder to use\n",
    "# If vectors are 768 dims, use Embedder_1; if 384 dims, use Embedder\n",
    "# Try Embedder first (384 dims), if that fails due to dimension mismatch, use Embedder_1\n",
    "\n",
    "try:\n",
    "    # Try with Embedder (384 dims)\n",
    "    Query_Vector = Embedder.encode (Query).tolist ()\n",
    "    Results = table.search(Query_Vector).limit(5).to_list ()\n",
    "except RuntimeError as e:\n",
    "    if \"dim\" in str(e) and \"doesn't match\" in str(e):\n",
    "        # Dimension mismatch - try with Embedder_1 (768 dims)\n",
    "        # Check if Embedder_1 exists, if not create it\n",
    "        try:\n",
    "            Embedder_1\n",
    "        except NameError:\n",
    "            Embedder_1 = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        \n",
    "        print(\"Note: Using Embedder_1 (768 dims) to match stored vectors\")\n",
    "        Query_Vector = Embedder_1.encode (Query).tolist ()\n",
    "        Results = table.search(Query_Vector).limit(5).to_list ()\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "for Rs in Results :\n",
    "\n",
    "    print (Rs['_distance'],\" ## \",Rs ['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Tech Repo**  \n",
    "> From various sources in internet, create a knowledge repo with all information chunked and vectorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "133\n",
      "134\n",
      "44\n",
      "75\n",
      "76\n",
      "39\n",
      "83\n",
      "45\n",
      "90\n",
      "91\n",
      "849\n"
     ]
    }
   ],
   "source": [
    "# Gather multiple reference material for Technology information\n",
    "References = [{'Source' : 'Microsoft', 'url' : \"https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-cloud-computing\"},\n",
    "              {'Source' : 'IBM', 'url' : \"https://www.ibm.com/think/topics/cloud-computing\"},\n",
    "              {'Source' : 'Oracle', 'url' : \"https://www.oracle.com/in/cloud/what-is-cloud-computing/\"},\n",
    "              {'Source' : 'AWS', 'url' : \"https://aws.amazon.com/what-is/iot/\"},\n",
    "              {'Source' : 'IBM', 'url' : \"https://www.ibm.com/think/topics/edge-ai\"},              \n",
    "              {'Source' : 'Microsoft', 'url' : \"https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-edge-computing\"},\n",
    "              {'Source' : 'IBM', 'url' : \"https://www.ibm.com/think/topics/edge-computing\"},              \n",
    "              {'Source' : 'Fortinet', 'url' : \"https://www.fortinet.com/resources/cyberglossary/edge-computing\"},\n",
    "              {'Source' : 'NVIDIA', 'url' : \"https://blogs.nvidia.com/blog/what-is-edge-ai/\"},\n",
    "              {'Source' : 'MIT', 'url' : \"https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained\"},\n",
    "              {'Source' : 'AWS', 'url' : \"https://aws.amazon.com/what-is/machine-learning/\"}\n",
    "            ]\n",
    "\n",
    "Chunks = []\n",
    "for Ref in References:\n",
    "    \n",
    "    Parts = Build_Chunks (Ref['url'], Ref ['Source'], 500)\n",
    "    Chunks = Chunks + Parts\n",
    "\n",
    "print (len(Chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedder_1 = SentenceTransformer (\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors and store in the Chunks \n",
    "for idx, Chunk in enumerate (Chunks):\n",
    "\n",
    "    vector = Embedder_1.encode (Chunk['text'])\n",
    "    Chunks[idx]['vector'] = vector.tolist ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: string\n",
      "topic: string\n",
      "text: string\n",
      "vector: fixed_size_list<item: float>[768]\n",
      "  child 0, item: float\n"
     ]
    }
   ],
   "source": [
    "# Connect to LanceDB (create connection if DB doesn't exist)\n",
    "DB = lancedb.connect('Vector_DB')\n",
    "\n",
    "# Create a Table and add the Chunks data\n",
    "table = DB.create_table(\"tech_ref\", data=Chunks, mode=\"overwrite\") \n",
    "print (table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM  ##  Edge computing is a distributed computing framework that brings enterprise applications closer to data sources such as IoT devices or local edge servers\n",
      "IBM  ##  This proximity to data at its source can deliver strong business benefits, including faster insights, improved response times and better bandwidth availability.  \n",
      "The explosive growth and increasing computing power of IoT devices has resulted in massive volumes of data\n",
      "IBM  ##  And data volumes continue to grow as 5G networks increase the number of connected mobile devices.  \n",
      "In the past, the promise of cloud and AI was to automate and speed up innovation by driving actionable insight from data\n",
      "IBM  ##  But connected devices create data at an extraordinary scale and complexity, outpacing network and infrastructure capabilities.  \n",
      "Sending all device-generated data to a centralized data center or to the cloud causes bandwidth and latency issues\n",
      "IBM  ##  Edge computing offers a more efficient alternative—data is processed and analyzed closer to the point where it's created. Because data does not traverse over a network to a cloud or data center to be processed, latency is reduced\n",
      "IBM  ##  Edge computing—and mobile edge computing on 5G networks—enables faster and more comprehensive data analysis, creating the opportunity for deeper insights, faster response times and improved customer experiences.  \n",
      "Industry newsletter\n",
      "Fortinet  ##  Edge computing involves positioning data storage and computation closer to where it is needed.\n"
     ]
    }
   ],
   "source": [
    "# Query a vector\n",
    "Query = \"There are many service providers\"\n",
    "\n",
    "Query_Vector = Embedder_1.encode (Query).tolist ()\n",
    "\n",
    "# Results = table.search(Query_Vector).distance_type(\"cosine\").limit(5).to_list ()\n",
    "Results = table.search().where(\"topic IN ('What is edge computing?')\").to_list ()\n",
    "\n",
    "for Rs in Results :\n",
    "\n",
    "    # print (Rs['_distance'],Rs['source'],\" ## \",Rs ['text'])\n",
    "    print (Rs['source'],\" ## \",Rs ['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_week5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
