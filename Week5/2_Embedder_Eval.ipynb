{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model Evaluation\n",
    "> Based on the use case / purpose, the embedding models can be evaluated with custom data  \n",
    "> The custom data can represent the industry / domain. It can represent the use case (classification / clustering / similarity etc)  \n",
    "> Depending on need, there can be multiple embedding model used within a system for different parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import of required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence transformers to use the embedding models locally\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Google AI library\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Load Environment variables from file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialise an client object with API key\n",
    "load_dotenv ()\n",
    "client = genai.Client()\n",
    "\n",
    "# Import library for making the tests\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Functions from SciPy for check / test\n",
    "from scipy import spatial\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions\n",
    "> Functions defined for basic comparison and test  \n",
    "> It can be re-used in different modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to provide cosine similarity between given 2 vectors.\n",
    "    Vectors are provided in the form of list of numbers\n",
    "    Cosine Similarity is returned as a number. 1 being the highest representing high similarity\n",
    "    \"\"\"\n",
    "\n",
    "    # spatial.distance.cosine returns the cosine distance\n",
    "    # from the distance similarity can be computed, considering the unit vector\n",
    "    \n",
    "    return 1 - spatial.distance.cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarities_HF(model, pairs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to compute similarity scores in each pair for a given list of text pairs.\n",
    "    Model from HF is given as an argument : as a sentence transformer model.\n",
    "    For the Text pairs provided, embedding vectors are captured and consine similarities are provided as a list of numbers\n",
    "    \"\"\"\n",
    "\n",
    "    similarities = []\n",
    "    \n",
    "    for s1, s2 in pairs:\n",
    "                \n",
    "        # Embed the texts\n",
    "        emb1 = model.encode(s1, convert_to_tensor=True)\n",
    "        emb2 = model.encode(s2, convert_to_tensor=True)\n",
    "        \n",
    "        # Identify the cosine similarity\n",
    "        sim = util.cos_sim(emb1, emb2).item()\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarities_gemini(model, pairs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to compute similarity scores in each pair for a given list of text pairs.\n",
    "    Model from Gemini is given as an argument : Name of the embedding model as string.\n",
    "    For the Text pairs provided, embedding vectors are captured and consine similarities are provided as a list of numbers    \n",
    "    \"\"\"\n",
    "\n",
    "    similarities = []\n",
    "    \n",
    "    for s1, s2 in pairs:\n",
    "        \n",
    "        result = client.models.embed_content(\n",
    "                model=model,        \n",
    "                contents=[s1, s2],\n",
    "                config=types.EmbedContentConfig(output_dimensionality=768)\n",
    "                )\n",
    "\n",
    "        sim = cosine_similarity (result.embeddings[0].values, result.embeddings[1].values)\n",
    "\n",
    "        similarities.append(sim)\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Similarity Test\n",
    "> Identify Known sentence pair, with known level of similarity  \n",
    "> Calculate similarity score for each pair from various model  \n",
    "> Compare the similarity scores from diff models for same text pair and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 2 embedders from HF to compare\n",
    "a = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "b = \"BAAI/bge-m3\"\n",
    "model_a = SentenceTransformer(a)\n",
    "model_b = SentenceTransformer(b)\n",
    "\n",
    "# Choose an embedding model from Gemini\n",
    "g = \"gemini-embedding-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of known text with expected similarity.\n",
    "sentence_pairs = [\n",
    "    (\"A man is eating pizza.\", \"A man is eating pizza.\"), # Same\n",
    "    (\"The weather was rainy last week.\", \"Currently I am wokring under hot sun\"), # opposite\n",
    "    (\"AI helps techies\", \"Techies actuall help AI\"), # opposite ..?\n",
    "    (\"He is playing flute.\", \"A person plays an instrument.\"),   # related\n",
    "    (\"I love dogs.\", \"She has a dog as pet.\"),      # somewhat related\n",
    "    (\"My car is red.\", \"My vehicle is painted Red\"),        # similar\n",
    "]\n",
    "\n",
    "# Compute similarity list from the chosen HF models for same set of texts\n",
    "sims_a = get_cosine_similarities_HF(model_a, sentence_pairs)\n",
    "sims_b = get_cosine_similarities_HF(model_b, sentence_pairs)\n",
    "\n",
    "# Compute similiarity list from the gemini model as well\n",
    "sims_g = get_cosine_similarities_gemini (g, sentence_pairs)\n",
    "\n",
    "# Result comparison\n",
    "Result = pd.DataFrame({\n",
    "    'Sentence 1': [p[0] for p in sentence_pairs],\n",
    "    'Sentence 2': [p[1] for p in sentence_pairs],\n",
    "    'Model A': sims_a,\n",
    "    'Model B': sims_b,\n",
    "    'Model G' : sims_g,\n",
    "})\n",
    "\n",
    "# Difference in similarity score between models\n",
    "print (\"Model A : \"+a+\"\\nModel B : \"+b+\"\\nModel G : \"+g)\n",
    "\n",
    "Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity Test : Data Set**  \n",
    "> Perform similarity test with pre defined data set, which is organised as text pairs and corresponding annotated score  \n",
    "> Its a sample data from a pre-defined HF dataset  \n",
    "> Similarly, a custom data set can be created, with annotated score Or enumerated range of score\n",
    "> Finally its compared with correlation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Test data from CSV file\n",
    "Similarity_Sample = pd.read_csv ('Similarity_Data_sample.csv')\n",
    "\n",
    "# Text pairs that are going to be used for similarity test\n",
    "Test_Pairs = list (zip (Similarity_Sample['sentence1'], Similarity_Sample['sentence2']))\n",
    "\n",
    "# Similarity score from Test Data as a reference\n",
    "Test_Sim = Similarity_Sample['score'].to_list()\n",
    "\n",
    "# Calculate pair wise similarity from 2 different models\n",
    "sims_a = get_cosine_similarities_HF(model_a, Test_Pairs)\n",
    "sims_b = get_cosine_similarities_HF(model_b, Test_Pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pearson Correlation**  \n",
    "It indicates how well the Test result (list of numbers) and the reference score (numbers) correlate  \n",
    "Higher the number well correlated. It means Test result of Similarity score is aligned to what is present in Reference data   \n",
    "Instead of absolute value check, the similarity scores can be enumerated based on range ( > 0.95 : Same; 0.8 .. 0.95 : Similar etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Pearson correlation : Model A\n",
    "corr_a, p_value_a = pearsonr(Test_Sim, sims_a)\n",
    "print(f\"Model A : Pearson correlation: {corr_a:.4f}\")\n",
    "\n",
    "# Compute Pearson correlation : Model B\n",
    "corr_b, p_value_b = pearsonr(Test_Sim, sims_b)\n",
    "print(f\"Model B : Pearson correlation: {corr_b:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Classification Test\n",
    "> When text are vectorised by embedding model, the resulting vectors (number array) can be treated as classification features  \n",
    "> If there is a pre-defined classification labels for the text, the corrsponding vectors from the model can be used for creating a classification model and a test can be made\n",
    "> This will indicate how well the vectors are grouped cohesively in the vector space for the given set of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Test data set with text and corresponding class label\n",
    "# this label can be representing various tags - depending on the use case\n",
    "texts = [\n",
    "        \"I love this movie!\",\n",
    "        \"This was terrible.\",\n",
    "        \"Amazing performance.\",\n",
    "        \"Not worth watching.\",\n",
    "        \"It was okay, not great.\",\n",
    "        \"Absolutely fantastic!\"\n",
    "]\n",
    "labels = ['Positive', 'Negative','Positive', 'Negative','Negative','Positive']\n",
    "\n",
    "# Split the data set for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Each model, make the embddings for both training data and test data  \n",
    "> Make a simple classification model, based on vector data and corresponding label for the Test data split  \n",
    "> Once the model is built, for the Test vectors predict the class  \n",
    "> make comparison for accuracy of class predicted vs the class label from the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models for comparison\n",
    "models = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\"\n",
    "]\n",
    "\n",
    "for name in models:\n",
    "    model = SentenceTransformer(name)\n",
    "    train_emb = model.encode(X_train)\n",
    "    test_emb = model.encode(X_test)\n",
    "\n",
    "    cl_model = LogisticRegression(max_iter=1000)\n",
    "    cl_model.fit(train_emb, y_train)\n",
    "    preds = cl_model.predict(test_emb)\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"{name} → Accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Set : Industry operation**  \n",
    "Pick a data set that represents a industry data.  \n",
    "Each text provided mentions a process step / operation / requirement from an industry  \n",
    "the classification label provides the department in the organisation which holds the responsibility / relevant to the process  \n",
    "Similarly vectorise the texts, train and test the classification model  \n",
    "The score can indicate how well vectors from 2 models can captured the nuance of the industry / organisation, when used in **classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sample data from CSV\n",
    "Data = pd.read_csv ('Process_n_Departments.csv')\n",
    "\n",
    "# Consider the text and label form the data frame\n",
    "texts = Data['Process'].to_list ()\n",
    "labels = Data['Department'].to_list ()\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualise**  \n",
    "> The vectors that are made from embedding model and the corresponding label can be used for visualisation  \n",
    "> High dimensionality vector cannot be visualised, unless it is reduced in dimension  \n",
    "> Store them as tab separated file and can be loaded in https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "Vectors = model.encode (texts, convert_to_numpy=True)\n",
    "\n",
    "# Write to tab-separated file\n",
    "\n",
    "with open('vectors.csv', \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    \n",
    "    for V in Vectors:\n",
    "        writer.writerow(V.tolist())\n",
    "\n",
    "with open('labels.csv', \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    \n",
    "    for L in labels:\n",
    "        writer.writerow([L])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Adv_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
