{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Chunking Strategies\n",
    "\n",
    "Text chunking is the process of breaking down large documents into smaller, manageable pieces. This is essential for:\n",
    "- **RAG (Retrieval Augmented Generation)**: Storing and retrieving relevant context\n",
    "- **Vector Databases**: Creating meaningful embeddings\n",
    "- **LLM Context Windows**: Fitting text within token limits\n",
    "- **Search & Information Retrieval**: Improving relevance and precision\n",
    "\n",
    "In this notebook, we'll explore fundamental chunking strategies with practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Text for Examples\n",
    "\n",
    "Let's create a sample document to demonstrate different chunking approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 968 characters\n",
      "Word count: 129 words\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Artificial intelligence has transformed the way we interact with technology. Machine learning algorithms can now recognize patterns in vast datasets, enabling applications from image recognition to natural language processing.\n",
    "\n",
    "Deep learning, a subset of machine learning, uses neural networks with multiple layers. These networks can learn hierarchical representations of data. The breakthrough came with increased computational power and large datasets.\n",
    "\n",
    "Natural language processing (NLP) allows computers to understand human language. Modern NLP models like transformers have revolutionized the field. They power applications such as chatbots, translation services, and text summarization.\n",
    "\n",
    "The future of AI involves more sophisticated models that can reason, plan, and interact with humans more naturally. Ethical considerations around AI deployment are becoming increasingly important. We must ensure AI systems are fair, transparent, and beneficial to society.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Original text length: {len(sample_text)} characters\")\n",
    "print(f\"Word count: {len(sample_text.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fixed-Size Chunking\n",
    "\n",
    "The simplest approach: split text into chunks of a fixed character or word count.\n",
    "\n",
    "**Pros:**\n",
    "- Simple to implement\n",
    "- Predictable chunk sizes\n",
    "- Fast processing\n",
    "\n",
    "**Cons:**\n",
    "- May split sentences or ideas mid-way\n",
    "- Doesn't respect document structure\n",
    "- Can break semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 7\n",
      "\n",
      "Chunk 1 (200 chars):\n",
      "Artificial intelligence has transformed the way we interact with technology. Machine learning algorithms can now recognize patterns in vast datasets, enabling applications from image recognition to na\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (200 chars):\n",
      "enabling applications from image recognition to natural language processing.\n",
      "\n",
      "Deep learning, a subset of machine learning, uses neural networks with multiple layers. These networks can learn hierarchi\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 (200 chars):\n",
      "ultiple layers. These networks can learn hierarchical representations of data. The breakthrough came with increased computational power and large datasets.\n",
      "\n",
      "Natural language processing (NLP) allows co\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4 (200 chars):\n",
      "sets.\n",
      "\n",
      "Natural language processing (NLP) allows computers to understand human language. Modern NLP models like transformers have revolutionized the field. They power applications such as chatbots, tra\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5 (200 chars):\n",
      "eld. They power applications such as chatbots, translation services, and text summarization.\n",
      "\n",
      "The future of AI involves more sophisticated models that can reason, plan, and interact with humans more n\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6 (200 chars):\n",
      " can reason, plan, and interact with humans more naturally. Ethical considerations around AI deployment are becoming increasingly important. We must ensure AI systems are fair, transparent, and benefi\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 7 (66 chars):\n",
      "nsure AI systems are fair, transparent, and beneficial to society.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def fixed_size_chunking(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into fixed-size chunks with optional overlap.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        chunk_size: Number of characters per chunk\n",
    "        overlap: Number of overlapping characters between chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text = text.strip()\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "fixed_chunks = fixed_size_chunking(sample_text, chunk_size=200, overlap=50)\n",
    "\n",
    "print(f\"Number of chunks: {len(fixed_chunks)}\\n\")\n",
    "for i, chunk in enumerate(fixed_chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence-Based Chunking\n",
    "\n",
    "Split text at sentence boundaries to preserve complete thoughts.\n",
    "\n",
    "**Pros:**\n",
    "- Preserves sentence integrity\n",
    "- More semantically meaningful\n",
    "- Better for understanding context\n",
    "\n",
    "**Cons:**\n",
    "- Variable chunk sizes\n",
    "- Requires sentence detection\n",
    "- May still break related ideas across chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 6\n",
      "\n",
      "Chunk 1:\n",
      "Artificial intelligence has transformed the way we interact with technology. Machine learning algorithms can now recognize patterns in vast datasets, enabling applications from image recognition to natural language processing.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2:\n",
      "Deep learning, a subset of machine learning, uses neural networks with multiple layers. These networks can learn hierarchical representations of data.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3:\n",
      "The breakthrough came with increased computational power and large datasets. Natural language processing (NLP) allows computers to understand human language.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4:\n",
      "Modern NLP models like transformers have revolutionized the field. They power applications such as chatbots, translation services, and text summarization.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5:\n",
      "The future of AI involves more sophisticated models that can reason, plan, and interact with humans more naturally. Ethical considerations around AI deployment are becoming increasingly important.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6:\n",
      "We must ensure AI systems are fair, transparent, and beneficial to society.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def sentence_chunking(text, sentences_per_chunk=3):\n",
    "    \"\"\"\n",
    "    Split text into chunks containing N sentences.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        sentences_per_chunk: Number of sentences per chunk\n",
    "    \"\"\"\n",
    "    # Simple sentence splitter (can be improved with spacy or nltk)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    sentences = [s for s in sentences if s]  # Remove empty strings\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = ' '.join(sentences[i:i + sentences_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "sentence_chunks = sentence_chunking(sample_text, sentences_per_chunk=2)\n",
    "\n",
    "print(f\"Number of chunks: {len(sentence_chunks)}\\n\")\n",
    "for i, chunk in enumerate(sentence_chunks, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Paragraph-Based Chunking\n",
    "\n",
    "Split text at paragraph boundaries (double newlines) to preserve topical coherence.\n",
    "\n",
    "**Pros:**\n",
    "- Preserves logical document structure\n",
    "- Keeps related ideas together\n",
    "- Natural semantic boundaries\n",
    "\n",
    "**Cons:**\n",
    "- Highly variable chunk sizes\n",
    "- Large paragraphs may exceed token limits\n",
    "- Depends on document formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 4\n",
      "\n",
      "Chunk 1 (226 chars):\n",
      "Artificial intelligence has transformed the way we interact with technology. Machine learning algorithms can now recognize patterns in vast datasets, enabling applications from image recognition to natural language processing.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (227 chars):\n",
      "Deep learning, a subset of machine learning, uses neural networks with multiple layers. These networks can learn hierarchical representations of data. The breakthrough came with increased computational power and large datasets.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 (235 chars):\n",
      "Natural language processing (NLP) allows computers to understand human language. Modern NLP models like transformers have revolutionized the field. They power applications such as chatbots, translation services, and text summarization.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4 (272 chars):\n",
      "The future of AI involves more sophisticated models that can reason, plan, and interact with humans more naturally. Ethical considerations around AI deployment are becoming increasingly important. We must ensure AI systems are fair, transparent, and beneficial to society.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def paragraph_chunking(text, max_paragraphs_per_chunk=2):\n",
    "    \"\"\"\n",
    "    Split text into chunks based on paragraphs.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        max_paragraphs_per_chunk: Maximum paragraphs per chunk\n",
    "    \"\"\"\n",
    "    # Split by double newlines or paragraph breaks\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(paragraphs), max_paragraphs_per_chunk):\n",
    "        chunk = '\\n\\n'.join(paragraphs[i:i + max_paragraphs_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "paragraph_chunks = paragraph_chunking(sample_text, max_paragraphs_per_chunk=1)\n",
    "\n",
    "print(f\"Number of chunks: {len(paragraph_chunks)}\\n\")\n",
    "for i, chunk in enumerate(paragraph_chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recursive Chunking\n",
    "\n",
    "A hierarchical approach that tries different separators in order (paragraphs → sentences → words → characters).\n",
    "\n",
    "**Pros:**\n",
    "- Balances semantic meaning with size constraints\n",
    "- More intelligent splitting\n",
    "- Adapts to document structure\n",
    "\n",
    "**Cons:**\n",
    "- More complex to implement\n",
    "- Slower than simple methods\n",
    "- Requires tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 4\n",
      "\n",
      "Chunk 1 (226 chars):\n",
      "Artificial intelligence has transformed the way we interact with technology. Machine learning algorithms can now recognize patterns in vast datasets, enabling applications from image recognition to natural language processing.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (227 chars):\n",
      "Deep learning, a subset of machine learning, uses neural networks with multiple layers. These networks can learn hierarchical representations of data. The breakthrough came with increased computational power and large datasets.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 (235 chars):\n",
      "Natural language processing (NLP) allows computers to understand human language. Modern NLP models like transformers have revolutionized the field. They power applications such as chatbots, translation services, and text summarization.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4 (272 chars):\n",
      "The future of AI involves more sophisticated models that can reason, plan, and interact with humans more naturally. Ethical considerations around AI deployment are becoming increasingly important. We must ensure AI systems are fair, transparent, and beneficial to society.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def recursive_chunking(text, chunk_size=300, overlap=50):\n",
    "    \"\"\"\n",
    "    Recursively split text using hierarchical separators.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        chunk_size: Target size for each chunk\n",
    "        overlap: Overlap between chunks\n",
    "    \"\"\"\n",
    "    # Separators in order of preference\n",
    "    separators = ['\\n\\n', '\\n', '. ', ' ', '']\n",
    "    \n",
    "    def split_text(text, separators):\n",
    "        chunks = []\n",
    "        \n",
    "        # Try each separator\n",
    "        for separator in separators:\n",
    "            if separator == '':\n",
    "                # Last resort: character-level split\n",
    "                return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-overlap)]\n",
    "            \n",
    "            if separator in text:\n",
    "                splits = text.split(separator)\n",
    "                current_chunk = \"\"\n",
    "                \n",
    "                for split in splits:\n",
    "                    if len(current_chunk) + len(split) + len(separator) <= chunk_size:\n",
    "                        current_chunk += split + separator\n",
    "                    else:\n",
    "                        if current_chunk:\n",
    "                            chunks.append(current_chunk.strip())\n",
    "                        current_chunk = split + separator\n",
    "                \n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                \n",
    "                return chunks\n",
    "        \n",
    "        return [text]\n",
    "    \n",
    "    return split_text(text.strip(), separators)\n",
    "\n",
    "# Example usage\n",
    "recursive_chunks = recursive_chunking(sample_text, chunk_size=300, overlap=30)\n",
    "\n",
    "print(f\"Number of chunks: {len(recursive_chunks)}\\n\")\n",
    "for i, chunk in enumerate(recursive_chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Token-Based Chunking\n",
    "\n",
    "Split text based on token count (important for LLM context windows).\n",
    "\n",
    "**Pros:**\n",
    "- Directly respects LLM token limits\n",
    "- Precise control over chunk sizes\n",
    "- Essential for API usage\n",
    "\n",
    "**Cons:**\n",
    "- Requires tokenizer library\n",
    "- Different models have different tokenizers\n",
    "- May still break semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken not installed. Install with: pip install tiktoken\n",
      "\n",
      "Token-based chunking requires the tiktoken library for accurate token counting.\n"
     ]
    }
   ],
   "source": [
    "# Note: This requires tiktoken library\n",
    "# pip install tiktoken\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    \n",
    "    def token_chunking(text, max_tokens=100, model=\"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Split text based on token count.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            max_tokens: Maximum tokens per chunk\n",
    "            model: Model name for tokenizer\n",
    "        \"\"\"\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        tokens = encoding.encode(text)\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), max_tokens):\n",
    "            chunk_tokens = tokens[i:i + max_tokens]\n",
    "            chunk_text = encoding.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    # Example usage\n",
    "    token_chunks = token_chunking(sample_text, max_tokens=50)\n",
    "    \n",
    "    print(f\"Number of chunks: {len(token_chunks)}\\n\")\n",
    "    for i, chunk in enumerate(token_chunks, 1):\n",
    "        print(f\"Chunk {i}:\")\n",
    "        print(chunk)\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"tiktoken not installed. Install with: pip install tiktoken\")\n",
    "    print(\"\\nToken-based chunking requires the tiktoken library for accurate token counting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Chunking Strategies\n",
    "\n",
    "Let's compare all the strategies we've covered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Strategy  Num Chunks  Avg Size  Min Size  Max Size\n",
      "     Fixed-Size           7       180        66       200\n",
      " Sentence-Based           6       159        75       226\n",
      "Paragraph-Based           4       240       226       272\n",
      "      Recursive           4       240       226       272\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Gather statistics\n",
    "strategies = {\n",
    "    'Fixed-Size': fixed_chunks,\n",
    "    'Sentence-Based': sentence_chunks,\n",
    "    'Paragraph-Based': paragraph_chunks,\n",
    "    'Recursive': recursive_chunks\n",
    "}\n",
    "\n",
    "comparison_data = []\n",
    "for name, chunks in strategies.items():\n",
    "    chunk_sizes = [len(c) for c in chunks]\n",
    "    comparison_data.append({\n",
    "        'Strategy': name,\n",
    "        'Num Chunks': len(chunks),\n",
    "        'Avg Size': int(sum(chunk_sizes) / len(chunk_sizes)),\n",
    "        'Min Size': min(chunk_sizes),\n",
    "        'Max Size': max(chunk_sizes)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Basic Chunking\n",
    "\n",
    "1. **Choose based on your use case:**\n",
    "   - RAG systems: Sentence or paragraph-based\n",
    "   - Token limits: Token-based chunking\n",
    "   - Simple processing: Fixed-size\n",
    "\n",
    "2. **Use overlap:** Helps maintain context between chunks (typically 10-20% of chunk size)\n",
    "\n",
    "3. **Consider chunk size:** \n",
    "   - Too small: Loses context\n",
    "   - Too large: Reduces retrieval precision\n",
    "   - Sweet spot: 256-512 tokens for most RAG applications\n",
    "\n",
    "4. **Preserve metadata:** Track source document, position, and relationships\n",
    "\n",
    "5. **Test and iterate:** Different documents may need different strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Ahead: Advanced Chunking Strategies\n",
    "\n",
    "The basic strategies we've covered are rule-based and don't understand the *meaning* of the text. In upcoming lessons, we'll explore **advanced chunking techniques** that leverage AI to create more intelligent chunks:\n",
    "\n",
    "### Semantic Chunking\n",
    "- Uses embeddings to understand text similarity\n",
    "- Groups sentences with related meanings together\n",
    "- Creates natural semantic boundaries\n",
    "- Better preserves context and improves retrieval accuracy\n",
    "\n",
    "### Agentic Chunking\n",
    "- Uses LLMs to intelligently identify logical boundaries\n",
    "- Can understand document structure and topic transitions\n",
    "- Adapts to different content types automatically\n",
    "\n",
    "### Context-Aware Chunking\n",
    "- Maintains references and relationships between chunks\n",
    "- Preserves hierarchical document structure\n",
    "- Creates \"smart\" overlaps based on content\n",
    "\n",
    "### Specialized Chunking\n",
    "- **Code chunking:** Respects function/class boundaries\n",
    "- **Markdown chunking:** Preserves heading hierarchy\n",
    "- **Table chunking:** Keeps tables intact\n",
    "\n",
    "These advanced techniques can significantly improve retrieval quality in RAG systems, but they come with trade-offs in complexity and computational cost. Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Try It Yourself!\n",
    "\n",
    "Experiment with different chunking strategies on your own text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-Size Chunks:\n",
      "1. Replace this with your own text to experiment with different chunking strategies.\n",
      "Try different chun\n",
      "\n",
      "2. .\n",
      "Try different chunk sizes, overlap values, and strategies to see what works best\n",
      "for your specific\n",
      "\n",
      "3. st\n",
      "for your specific use case.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sentence Chunks:\n",
      "1. Replace this with your own text to experiment with different chunking strategies. Try different chunk sizes, overlap values, and strategies to see what works best\n",
      "for your specific use case.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add your own text here\n",
    "your_text = \"\"\"\n",
    "Replace this with your own text to experiment with different chunking strategies.\n",
    "Try different chunk sizes, overlap values, and strategies to see what works best\n",
    "for your specific use case.\n",
    "\"\"\"\n",
    "\n",
    "# Try different strategies\n",
    "print(\"Fixed-Size Chunks:\")\n",
    "for i, chunk in enumerate(fixed_size_chunking(your_text, 100, 20), 1):\n",
    "    print(f\"{i}. {chunk}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Sentence Chunks:\")\n",
    "for i, chunk in enumerate(sentence_chunking(your_text, 2), 1):\n",
    "    print(f\"{i}. {chunk}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
